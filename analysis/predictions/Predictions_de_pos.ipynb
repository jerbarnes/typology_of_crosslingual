{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from zero_shot.zero_shot import Tester\n",
    "from utils import pos_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = pos_utils.get_ud_tags()\n",
    "params = {\n",
    "    \"data_path\": \"../../data/\",\n",
    "    \"results_path\": None,\n",
    "    \"short_model_name\": \"mbert\",\n",
    "    \"task\": \"pos\",\n",
    "    \"checkpoint_dir\": \"E:/TFM_CCIL/checkpoints_colab/\",\n",
    "    \"experiment\": \"acl\",\n",
    "    \"max_length\": 256,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_labels\": len(tagset),\n",
    "    \"tagset\": tagset\n",
    "}\n",
    "\n",
    "tester = Tester(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tester.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights from E:/TFM_CCIL/checkpoints_colab/de/bert-base-multilingual-cased_pos.hdf5\n"
     ]
    }
   ],
   "source": [
    "tester.set_model_lang(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dataset, batches = tester.load_data(\"../../data/ud/no/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 31s 1s/step\n"
     ]
    }
   ],
   "source": [
    "preds = tester.model.predict(dataset, steps=batches, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_preds = preds[0].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          True      Predicted \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Om             SCONJ     SCONJ     \n",
      "ikke           PART      PART      \n",
      "Elsa           PROPN     PROPN     \n",
      "f√•r            VERB      VERB      \n",
      "i              ADP       ADP       \n",
      "seg            PRON      PRON      \n",
      "nok            ADV       ADJ       \n",
      "Omega-3        PROPN     PROPN     \n",
      "i              ADP       PROPN     \n",
      "et             DET       PROPN     \n",
      "naturlig       ADJ       ADP       \n",
      "kosthold       NOUN      DET       \n",
      ",              PUNCT     ADJ       \n",
      "har            VERB      ADJ       \n",
      "jeg            PRON      ADJ       \n",
      "et             DET       ADJ       \n",
      "problem        NOUN      PUNCT     \n",
      ".              PUNCT     AUX       \n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(data))\n",
    "\n",
    "sentence_tokens = data[i][\"tokens\"]\n",
    "sentence_tags = data[i][\"tags\"]\n",
    "sentence_preds = [tagset[x] for x in clean_preds[i]]\n",
    "print(\"{:<15}{:<10}{:<10}\".format(\"Token\", \"True\", \"Predicted\"))\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for j in range(len(sentence_tokens)):\n",
    "    print(\"{:<15}{:<10}{:<10}\".format(sentence_tokens[j], sentence_tags[j], sentence_preds[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
