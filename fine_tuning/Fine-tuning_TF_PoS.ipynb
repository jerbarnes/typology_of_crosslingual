{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFBertForTokenClassification, TFXLMRobertaForTokenClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import MBERT_Tokenizer, XLMR_Tokenizer, read_conll\n",
    "import utils.utils as utils\n",
    "import utils.pos_utils as pos_utils\n",
    "import utils.fine_tuning_utils as fine_tune_utils\n",
    "import utils.model_utils as model_utils\n",
    "import data_preparation.data_preparation_pos as data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language and general setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No languages remaining \n",
      "\n",
      "Already trained:     Bulgarian  English  Russian  Slovak  Croatian  Chinese  Vietnamese  Finnish  Basque  Japanese  Korean  Turkish  Arabic  Hebrew\n",
      "\n",
      "Cannot train:        Thai\n",
      "\n",
      "Retrain language? y\n",
      "Language to re-train: English\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/ud/\"\n",
    "short_model_name = \"xlm-roberta\"\n",
    "experiment = \"tfm\"\n",
    "task = \"pos\"\n",
    "checkpoints_path = \"E:/TFM_CCIL/checkpoints/\"\n",
    "        \n",
    "training_lang = fine_tune_utils.get_global_training_state(data_path, short_model_name, experiment, checkpoints_path)\n",
    "trainer = fine_tune_utils.Trainer(training_lang, data_path, task, short_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-xlm-roberta-base were not used when initializing TFXLMRobertaForTokenClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFXLMRobertaForTokenClassification were not initialized from the model checkpoint at jplu/tf-xlm-roberta-base and are newly initialized: ['dropout_38', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built tf-xlm-roberta-base\n",
      "Checkpoint file: E:/TFM_CCIL/checkpoints/en/tf-xlm-roberta-base_pos_checkpoint.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "max_length = 256\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "epochs = 20\n",
    "tagset = pos_utils.get_ud_tags()\n",
    "num_labels = len(tagset)\n",
    "\n",
    "# Model creation\n",
    "trainer.build_model(max_length, batch_size, learning_rate, epochs, num_labels, tagset=tagset, eval_batch_size=64)\n",
    "\n",
    "# Checkpoint for best model weights\n",
    "trainer.setup_checkpoint(checkpoints_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e934d795294bd2a77c9d89f245afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train examples: 4287\n",
      "\"                        PUNCT               \n",
      "Get                      VERB                \n",
      "out                      ADP                 \n",
      "of                       ADP                 \n",
      "here                     ADV                 \n",
      "\"                        PUNCT               \n",
      "meant                    VERB                \n",
      "to                       PART                \n",
      "make                     VERB                \n",
      "someone                  NOUN                \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "trainer.prepare_data()\n",
    "\n",
    "print(\"Train examples:\", len(trainer.train_data))\n",
    "\n",
    "# Print an example sentence for sanity\n",
    "example_batch = trainer.train_dataset.as_numpy_iterator().next()\n",
    "for token, label in zip(example_batch[0][\"input_ids\"][0], example_batch[1][0]):\n",
    "    if not token:\n",
    "        break\n",
    "    elif token == example_batch[0][\"input_ids\"][0][10]:\n",
    "        print(\"...\")\n",
    "        break\n",
    "    print(\"{:<25}{:<20}\".format(trainer.tokenizer.decode(int(token)), tagset[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_dir + model_name + \"_pos_checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.2835 - ignore_acc: 0.7016\n",
      "Epoch 00001: val_ignore_acc improved from -inf to 0.93807, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 424s 646ms/step - loss: 0.2835 - ignore_acc: 0.7016 - val_loss: 0.0417 - val_ignore_acc: 0.9381\n",
      "Epoch 2/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0399 - ignore_acc: 0.9515\n",
      "Epoch 00002: val_ignore_acc improved from 0.93807 to 0.95742, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 428s 653ms/step - loss: 0.0399 - ignore_acc: 0.9515 - val_loss: 0.0270 - val_ignore_acc: 0.9574\n",
      "Epoch 3/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0277 - ignore_acc: 0.9640\n",
      "Epoch 00003: val_ignore_acc improved from 0.95742 to 0.96128, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 428s 652ms/step - loss: 0.0277 - ignore_acc: 0.9640 - val_loss: 0.0237 - val_ignore_acc: 0.9613\n",
      "Epoch 4/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0242 - ignore_acc: 0.9687\n",
      "Epoch 00004: val_ignore_acc improved from 0.96128 to 0.96267, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 425s 647ms/step - loss: 0.0242 - ignore_acc: 0.9687 - val_loss: 0.0229 - val_ignore_acc: 0.9627\n",
      "Epoch 5/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0187 - ignore_acc: 0.9729\n",
      "Epoch 00005: val_ignore_acc improved from 0.96267 to 0.96349, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 425s 648ms/step - loss: 0.0187 - ignore_acc: 0.9729 - val_loss: 0.0228 - val_ignore_acc: 0.9635\n",
      "Epoch 6/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0158 - ignore_acc: 0.9774\n",
      "Epoch 00006: val_ignore_acc improved from 0.96349 to 0.96696, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 427s 651ms/step - loss: 0.0158 - ignore_acc: 0.9774 - val_loss: 0.0208 - val_ignore_acc: 0.9670\n",
      "Epoch 7/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0120 - ignore_acc: 0.9832\n",
      "Epoch 00007: val_ignore_acc improved from 0.96696 to 0.97197, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 426s 649ms/step - loss: 0.0120 - ignore_acc: 0.9832 - val_loss: 0.0202 - val_ignore_acc: 0.9720\n",
      "Epoch 8/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0099 - ignore_acc: 0.9862\n",
      "Epoch 00008: val_ignore_acc improved from 0.97197 to 0.97379, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 423s 645ms/step - loss: 0.0099 - ignore_acc: 0.9862 - val_loss: 0.0195 - val_ignore_acc: 0.9738\n",
      "Epoch 9/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0085 - ignore_acc: 0.9878\n",
      "Epoch 00009: val_ignore_acc did not improve from 0.97379\n",
      "656/656 [==============================] - 416s 634ms/step - loss: 0.0085 - ignore_acc: 0.9878 - val_loss: 0.0201 - val_ignore_acc: 0.9726\n",
      "Epoch 10/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0074 - ignore_acc: 0.9898\n",
      "Epoch 00010: val_ignore_acc improved from 0.97379 to 0.97424, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0074 - ignore_acc: 0.9898 - val_loss: 0.0209 - val_ignore_acc: 0.9742\n",
      "Epoch 11/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0069 - ignore_acc: 0.9904\n",
      "Epoch 00011: val_ignore_acc improved from 0.97424 to 0.97461, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 424s 646ms/step - loss: 0.0069 - ignore_acc: 0.9904 - val_loss: 0.0206 - val_ignore_acc: 0.9746\n",
      "Epoch 12/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0063 - ignore_acc: 0.9911\n",
      "Epoch 00012: val_ignore_acc improved from 0.97461 to 0.97538, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 427s 650ms/step - loss: 0.0063 - ignore_acc: 0.9911 - val_loss: 0.0197 - val_ignore_acc: 0.9754\n",
      "Epoch 13/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0052 - ignore_acc: 0.9927\n",
      "Epoch 00013: val_ignore_acc improved from 0.97538 to 0.97708, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 426s 650ms/step - loss: 0.0052 - ignore_acc: 0.9927 - val_loss: 0.0197 - val_ignore_acc: 0.9771\n",
      "Epoch 14/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0049 - ignore_acc: 0.9930\n",
      "Epoch 00014: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 419s 639ms/step - loss: 0.0049 - ignore_acc: 0.9930 - val_loss: 0.0218 - val_ignore_acc: 0.9741\n",
      "Epoch 15/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0052 - ignore_acc: 0.9928\n",
      "Epoch 00015: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0052 - ignore_acc: 0.9928 - val_loss: 0.0204 - val_ignore_acc: 0.9757\n",
      "Epoch 16/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0043 - ignore_acc: 0.9940\n",
      "Epoch 00016: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0043 - ignore_acc: 0.9940 - val_loss: 0.0219 - val_ignore_acc: 0.9746\n",
      "Epoch 17/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0053 - ignore_acc: 0.9928\n",
      "Epoch 00017: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 642ms/step - loss: 0.0053 - ignore_acc: 0.9928 - val_loss: 0.0221 - val_ignore_acc: 0.9749\n",
      "Epoch 18/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0031 - ignore_acc: 0.9956\n",
      "Epoch 00018: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 422s 644ms/step - loss: 0.0031 - ignore_acc: 0.9956 - val_loss: 0.0255 - val_ignore_acc: 0.9745\n",
      "Epoch 19/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0027 - ignore_acc: 0.9961\n",
      "Epoch 00019: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 420s 640ms/step - loss: 0.0027 - ignore_acc: 0.9961 - val_loss: 0.0225 - val_ignore_acc: 0.9756\n",
      "Epoch 20/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0028 - ignore_acc: 0.9960\n",
      "Epoch 00020: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 420s 640ms/step - loss: 0.0028 - ignore_acc: 0.9960 - val_loss: 0.0243 - val_ignore_acc: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xc6b49c74e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=epochs, steps_per_epoch=np.ceil(len(train_examples) / batch_size),\n",
    "          validation_data=dev_dataset, validation_steps=np.ceil(len(dev_examples) / batch_size),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
