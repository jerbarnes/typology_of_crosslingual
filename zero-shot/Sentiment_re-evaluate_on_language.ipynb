{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_sentiment import Example, convert_examples_to_tf_dataset, make_batches\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dicts = utils.make_lang_code_dicts()\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "results_path = \"../results/results_sentiment.xlsx\"\n",
    "\n",
    "target_lang = \"Slovak\"\n",
    "target_lang = name_to_code[target_lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "max_length = 512\n",
    "batch_size = 64\n",
    "\n",
    "# Model creation and loading weights\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess\n",
    "test = pd.read_csv(\"../data/sentiment/\" + target_lang + \"/test.csv\", header=None)\n",
    "test.columns = [\"sentiment\", \"review\"]\n",
    "lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "test = test[lengths <= 512].reset_index(drop=True) # Remove long examples\n",
    "\n",
    "# Convert to TF dataset\n",
    "test_dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                               text in test.values], \n",
    "                                              tokenizer, max_length=max_length)\n",
    "test_dataset, test_batches = make_batches(test_dataset, batch_size, repetitions=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d26ab8f34c04d1bbe024fc9465fccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 38s 2s/step\n",
      "17/17 [==============================] - 38s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "17/17 [==============================] - 40s 2s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_eval = []\n",
    "\n",
    "for weights_filename in tqdm(glob.glob(\"E:/TFM_CCIL/checkpoints/*/*sentiment.hdf5\")):\n",
    "    # Load weights for training language\n",
    "    lang = weights_filename.split(\"\\\\\")[-2]\n",
    "    model.load_weights(weights_filename)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(test_dataset, steps=np.ceil(test.shape[0] / batch_size), verbose=1)\n",
    "    clean_preds = preds[0].argmax(axis=-1)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(test[\"sentiment\"].values, clean_preds)\n",
    "    precision = precision_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "    recall = recall_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "    f1 = f1_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "    sentiment_eval.append((lang, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table for this testing language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Bulgarian</th>\n",
       "      <th>English</th>\n",
       "      <th>Basque</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Hebrew</th>\n",
       "      <th>Croatian</th>\n",
       "      <th>Slovak</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.718045</td>\n",
       "      <td>0.844925</td>\n",
       "      <td>0.900376</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.87218</td>\n",
       "      <td>0.953947</td>\n",
       "      <td>0.347744</td>\n",
       "      <td>0.800752</td>\n",
       "      <td>0.737782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macro_Precision</td>\n",
       "      <td>0.558827</td>\n",
       "      <td>0.63208</td>\n",
       "      <td>0.60013</td>\n",
       "      <td>0.796236</td>\n",
       "      <td>0.653602</td>\n",
       "      <td>0.704619</td>\n",
       "      <td>0.888882</td>\n",
       "      <td>0.558079</td>\n",
       "      <td>0.633757</td>\n",
       "      <td>0.504236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macro_Recall</td>\n",
       "      <td>0.538757</td>\n",
       "      <td>0.786734</td>\n",
       "      <td>0.570363</td>\n",
       "      <td>0.67088</td>\n",
       "      <td>0.734134</td>\n",
       "      <td>0.707297</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.598553</td>\n",
       "      <td>0.715782</td>\n",
       "      <td>0.505989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macro_F1</td>\n",
       "      <td>0.54351</td>\n",
       "      <td>0.623114</td>\n",
       "      <td>0.580071</td>\n",
       "      <td>0.710106</td>\n",
       "      <td>0.676623</td>\n",
       "      <td>0.705944</td>\n",
       "      <td>0.895075</td>\n",
       "      <td>0.338467</td>\n",
       "      <td>0.653218</td>\n",
       "      <td>0.50092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric Bulgarian   English    Basque   Finnish    Hebrew  \\\n",
       "0         Accuracy  0.835526  0.718045  0.844925  0.900376  0.821429   \n",
       "1  Macro_Precision  0.558827   0.63208   0.60013  0.796236  0.653602   \n",
       "2     Macro_Recall  0.538757  0.786734  0.570363   0.67088  0.734134   \n",
       "3         Macro_F1   0.54351  0.623114  0.580071  0.710106  0.676623   \n",
       "\n",
       "   Croatian    Slovak      Thai Vietnamese   Chinese  \n",
       "0   0.87218  0.953947  0.347744   0.800752  0.737782  \n",
       "1  0.704619  0.888882  0.558079   0.633757  0.504236  \n",
       "2  0.707297  0.901561  0.598553   0.715782  0.505989  \n",
       "3  0.705944  0.895075  0.338467   0.653218   0.50092  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_eval = np.array(sentiment_eval, dtype=object)\n",
    "table = pd.DataFrame({**{\"Metric\": [\"Accuracy\", \"Macro_Precision\", \"Macro_Recall\", \"Macro_F1\"]},\n",
    "                      **{code_to_name[sentiment_eval[i,0]]: sentiment_eval[i,1:] for i in range(sentiment_eval.shape[0])}})\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(results_path, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(results_path) as writer:\n",
    "    for sheet_name, df in results.items():\n",
    "        # Make sure the same training languages are present\n",
    "        current_langs = [col_name for col_name in table.columns if (table[col_name].apply(lambda x: \n",
    "                                                                    isinstance(x, (np.floating, float))).all())]\n",
    "        current_langs.sort()\n",
    "        file_langs = [col_name for col_name in df.columns if (df[col_name].apply(lambda x: \n",
    "                                                              isinstance(x, (np.floating, float))).all())]\n",
    "        file_langs.sort()\n",
    "        assert current_langs == file_langs, \"Language mismatch between table and results file\"\n",
    "\n",
    "        # Update values in testing language row\n",
    "        df.update(pd.DataFrame(table.loc[table[\"Metric\"] == sheet_name, current_langs].to_dict(\"list\"),\n",
    "                               index=[df.index[df[\"Language\"] == code_to_name[target_lang]][0]]))\n",
    "        \n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
