{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_sentiment import Example, convert_examples_to_tf_dataset, make_batches\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with:    Thai \n",
      "\n",
      "Already evaluated:  Bulgarian  English  Basque  Finnish  Hebrew  Croatian  Slovak  Vietnamese  Chinese\n",
      "\n",
      "Not yet evaluated:\n",
      "\n",
      "Still to train:     Korean  Arabic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_dicts = utils.make_lang_code_dicts(\"../utils/lang_codes.xlsx\")\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "results_path = \"../results/results_sentiment.xlsx\"\n",
    "\n",
    "# Look for languages that have sentiment weights but are not in the results file\n",
    "file = open(\"../data_exploration/sentiment_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "all_langs = [lang for lang in all_langs if lang not in [\"Turkish\", \"Japanese\", \"Russian\"]]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/*sentiment.hdf5\")]\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "    remaining_langs = [lang for lang in trained_langs if lang not in results[\"Accuracy\"].columns]\n",
    "else:\n",
    "    remaining_langs = trained_langs\n",
    "    \n",
    "untrained_langs = [lang for lang in all_langs if lang not in trained_langs]\n",
    "evaluated_langs = [lang for lang in trained_langs if lang not in remaining_langs]\n",
    "\n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"Evaluating with:   \", training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet evaluated:\"] + remaining_langs[1:], displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\")\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights from E:/TFM_CCIL/checkpoints/zh/bert-base-multilingual-cased_sentiment.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "max_length = 512\n",
    "batch_size = 64\n",
    "\n",
    "# Model creation and loading weights\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "weights_path = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "weights_filename = model_name + \"_sentiment.hdf5\"\n",
    "model.load_weights(weights_path + weights_filename)\n",
    "print(\"Using weights from\", weights_path + weights_filename)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6beafeae6cb44855ac08be6b5be874d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 382s 3s/step\n",
      "29/29 [==============================] - 72s 2s/step\n",
      "29/29 [==============================] - 71s 2s/step\n",
      "4/4 [==============================] - 7s 2s/step\n",
      "7/7 [==============================] - 13s 2s/step\n",
      "28/28 [==============================] - 69s 2s/step\n",
      "7/7 [==============================] - 15s 2s/step\n",
      "54/54 [==============================] - 136s 3s/step\n",
      "17/17 [==============================] - 41s 2s/step\n",
      "37/37 [==============================] - 92s 2s/step\n",
      "11/11 [==============================] - 25s 2s/step\n",
      "86/86 [==============================] - 220s 3s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "sentiment_eval = []\n",
    "\n",
    "for lang in tqdm(os.listdir(path)):\n",
    "    if lang not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        # Load and preprocess\n",
    "        test = pd.read_csv(path + lang + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "        test = test[lengths <= 512].reset_index(drop=True) # Remove long examples\n",
    "        \n",
    "        # Convert to TF dataset\n",
    "        test_dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                                       text in test.values], \n",
    "                                                      tokenizer, max_length=max_length)\n",
    "        test_dataset, test_batches = make_batches(test_dataset, batch_size, repetitions=1, shuffle=False)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(test_dataset, steps=np.ceil(test.shape[0] / batch_size), verbose=1)\n",
    "        clean_preds = preds[0].argmax(axis=-1)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(test[\"sentiment\"].values, clean_preds)\n",
    "        precision = precision_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "        recall = recall_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "        f1 = f1_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "        sentiment_eval.append((lang, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table for this training language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_eval = np.array(sentiment_eval, dtype=object)\n",
    "table = pd.DataFrame({\"Language\": sentiment_eval[:,0],\n",
    "                      \"Accuracy\": sentiment_eval[:,1],\n",
    "                      \"Macro_Precision\": sentiment_eval[:,2],\n",
    "                      \"Macro_Recall\": sentiment_eval[:,3],\n",
    "                      \"Macro_F1\": sentiment_eval[:,4]})\n",
    "table[\"Language\"] = table[\"Language\"].apply(lambda x: code_to_name[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder so that language types are grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data_exploration/sentiment_table.txt\", \"r\")\n",
    "lang_order = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "lang_order = [lang for lang in lang_order if lang not in [\"Turkish\", \"Japanese\", \"Russian\"]]\n",
    "table[\"sort\"] = table[\"Language\"].apply(lambda x: lang_order.index(x))\n",
    "table = table.sort_values(by=[\"sort\"]).drop(\"sort\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>0.456723</td>\n",
       "      <td>0.562093</td>\n",
       "      <td>0.591042</td>\n",
       "      <td>0.443895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>0.584843</td>\n",
       "      <td>0.586812</td>\n",
       "      <td>0.584716</td>\n",
       "      <td>0.582267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Slovak</td>\n",
       "      <td>0.760338</td>\n",
       "      <td>0.495069</td>\n",
       "      <td>0.489536</td>\n",
       "      <td>0.480364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Croatian</td>\n",
       "      <td>0.727689</td>\n",
       "      <td>0.564179</td>\n",
       "      <td>0.548601</td>\n",
       "      <td>0.551284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.932025</td>\n",
       "      <td>0.926653</td>\n",
       "      <td>0.932968</td>\n",
       "      <td>0.929456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.524553</td>\n",
       "      <td>0.522831</td>\n",
       "      <td>0.51597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thai</td>\n",
       "      <td>0.580908</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>0.53807</td>\n",
       "      <td>0.529074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>0.747449</td>\n",
       "      <td>0.609397</td>\n",
       "      <td>0.518707</td>\n",
       "      <td>0.480503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Basque</td>\n",
       "      <td>0.585903</td>\n",
       "      <td>0.518059</td>\n",
       "      <td>0.533259</td>\n",
       "      <td>0.483691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Korean</td>\n",
       "      <td>0.329144</td>\n",
       "      <td>0.421348</td>\n",
       "      <td>0.457106</td>\n",
       "      <td>0.306814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>0.307373</td>\n",
       "      <td>0.515205</td>\n",
       "      <td>0.516934</td>\n",
       "      <td>0.306935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hebrew</td>\n",
       "      <td>0.300339</td>\n",
       "      <td>0.502688</td>\n",
       "      <td>0.514076</td>\n",
       "      <td>0.25743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language  Accuracy Macro_Precision Macro_Recall  Macro_F1\n",
       "0    Bulgarian  0.456723        0.562093     0.591042  0.443895\n",
       "1      English  0.584843        0.586812     0.584716  0.582267\n",
       "2       Slovak  0.760338        0.495069     0.489536  0.480364\n",
       "3     Croatian  0.727689        0.564179     0.548601  0.551284\n",
       "4      Chinese  0.932025        0.926653     0.932968  0.929456\n",
       "5   Vietnamese  0.526316        0.524553     0.522831   0.51597\n",
       "6         Thai  0.580908        0.547758      0.53807  0.529074\n",
       "7      Finnish  0.747449        0.609397     0.518707  0.480503\n",
       "8       Basque  0.585903        0.518059     0.533259  0.483691\n",
       "9       Korean  0.329144        0.421348     0.457106  0.306814\n",
       "10      Arabic  0.307373        0.515205     0.516934  0.306935\n",
       "11      Hebrew  0.300339        0.502688     0.514076   0.25743"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update results excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/results_sentiment.xlsx\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "else:\n",
    "    results = dict.fromkeys(table.columns[1:].values, pd.DataFrame({\"Language\": table[\"Language\"].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(results_path) as writer:\n",
    "    full_training_lang = code_to_name[training_lang]\n",
    "    for sheet_name, df in results.items():\n",
    "        # Add each the column for each metric in the corresponding sheet\n",
    "        df[full_training_lang] = table[sheet_name]\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking dataset balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fc7e8c1f6f4bbeb443aa9149edc64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar 0.8325724493846764\n",
      "bg 0.8111050626020686\n",
      "en 0.49917627677100496\n",
      "eu 0.8458149779735683\n",
      "fi 0.7455919395465995\n",
      "he 0.040654997176736304\n",
      "hr 0.7803203661327232\n",
      "ko 2.8757886435331232\n",
      "sl 0.9219924812030075\n",
      "th 0.40784982935153585\n",
      "vi 0.5138686131386861\n",
      "zh 0.6045710139669871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "\n",
    "for lang in tqdm(os.listdir(path)):\n",
    "    if lang not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        test = pd.read_csv(path + lang + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        print(lang, test[\"sentiment\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating excluded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60faaaf0acf44f8b593722db3d51f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "max_lengths = {}\n",
    "for directory in tqdm(os.listdir(path)):\n",
    "    lang_path = os.path.join(path, directory)\n",
    "    test = pd.read_csv(lang_path + \"/test.csv\", header=None)\n",
    "    test.columns = [\"sentiment\", \"review\"]\n",
    "    lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    above_512 = lengths > 512\n",
    "    above_256 = lengths > 256\n",
    "    max_lengths[directory] = (lengths.max(), \n",
    "                              above_512.sum(), round(above_512.mean() * 100, 2), \n",
    "                              above_256.sum(), round(above_256.mean() * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': (5040, 446, 4.43, 1242, 12.33),\n",
       " 'bg': (65, 0, 0.0, 0, 0.0),\n",
       " 'en': (77, 0, 0.0, 0, 0.0),\n",
       " 'eu': (79, 0, 0.0, 0, 0.0),\n",
       " 'fi': (657, 5, 1.26, 26, 6.55),\n",
       " 'he': (1183, 3, 0.17, 16, 0.9),\n",
       " 'hr': (316, 0, 0.0, 1, 0.23),\n",
       " 'ja': (6836, 685, 34.25, 1677, 83.85),\n",
       " 'ko': (212, 0, 0.0, 0, 0.0),\n",
       " 'ru': (9751, 496, 57.21, 750, 86.51),\n",
       " 'sk': (411, 0, 0.0, 9, 0.85),\n",
       " 'th': (1336, 8, 0.34, 48, 2.05),\n",
       " 'tr': (2055, 184, 100.0, 184, 100.0),\n",
       " 'vi': (826, 1, 0.15, 3, 0.44),\n",
       " 'zh': (847, 11, 0.2, 106, 1.92)}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language            Longest             >512 tokens         >512 tokens (%)     >256 tokens         >256 tokens (%)     \n",
      "\n",
      "ar                  5040                446                 4.43                1242                12.33               \n",
      "bg                  65                  0                   0.0                 0                   0.0                 \n",
      "en                  77                  0                   0.0                 0                   0.0                 \n",
      "eu                  79                  0                   0.0                 0                   0.0                 \n",
      "fi                  657                 5                   1.26                26                  6.55                \n",
      "he                  1183                3                   0.17                16                  0.9                 \n",
      "hr                  316                 0                   0.0                 1                   0.23                \n",
      "ja                  6836                685                 34.25               1677                83.85               \n",
      "ko                  212                 0                   0.0                 0                   0.0                 \n",
      "ru                  9751                496                 57.21               750                 86.51               \n",
      "sk                  411                 0                   0.0                 9                   0.85                \n",
      "th                  1336                8                   0.34                48                  2.05                \n",
      "tr                  2055                184                 100.0               184                 100.0               \n",
      "vi                  826                 1                   0.15                3                   0.44                \n",
      "zh                  847                 11                  0.2                 106                 1.92                \n"
     ]
    }
   ],
   "source": [
    "print((\"{:<20}\" * 6).format(\"Language\", \"Longest\", \">512 tokens\", \">512 tokens (%)\", \n",
    "                                        \">256 tokens\", \">256 tokens (%)\") + \"\\n\")\n",
    "for lang, values in max_lengths.items():\n",
    "    print((\"{:<20}\" * 6).format(*([lang] + list(values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "output = \"\"\n",
    "lang_codes = pd.read_excel(\"../data_exploration/lang_codes.xlsx\", header=0)\n",
    "\n",
    "for line in file.readlines():\n",
    "    lang_name = line.split(\"&\")[1].strip()\n",
    "    lang_code = lang_codes[\"ISO 639-1 Code\"][lang_codes[\"English name of Language\"] == lang_name].values[0]\n",
    "    \n",
    "    if lang_code in max_lengths:\n",
    "        values = max_lengths[lang_code]\n",
    "        split_line = line.split(\"\\\\\")\n",
    "        start = split_line[0] + \"\\\\\" + \"&\".join(split_line[1].split(\"&\")[:2])\n",
    "        end = r\"\\\\\" + \"\".join(split_line[2:])\n",
    "        new_line = start + \"& \" + \" & \".join(np.array(values[1:]).astype(str)) + end\n",
    "        \n",
    "    else:\n",
    "        new_line = line\n",
    "        \n",
    "    output += new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \\fusional{Fusional}  & Bulgarian & 0.0 & 0.0 & 0.0 & 0.0\\\\ \n",
      "    \\fusional{Fusional} & English & 0.0 & 0.0 & 0.0 & 0.0\\\\\n",
      "    \\fusional{Fusional}  & Russian & 496.0 & 57.21 & 750.0 & 86.51\\\\ \n",
      "    \\fusional{Fusional} & Slovak & 0.0 & 0.0 & 9.0 & 0.85\\\\\n",
      "    \\fusional{Fusional}  & Croatian & 0.0 & 0.0 & 1.0 & 0.23\\\\\n",
      "    \\isolating{Isolating} & Chinese & 11.0 & 0.2 & 106.0 & 1.92\\\\ \n",
      "    \\isolating{Isolating} & Vietnamese  & 1.0 & 0.15 & 3.0 & 0.44\\\\\n",
      "    \\isolating{Isolating} & Thai & 8.0 & 0.34 & 48.0 & 2.05\\\\\n",
      "    \\agglutinative{Agglutinative} & Finnish & 5.0 & 1.26 & 26.0 & 6.55\\\\ \n",
      "    \\agglutinative{Agglutinative} & Basque & 0.0 & 0.0 & 0.0 & 0.0\\\\\n",
      "    \\agglutinative{Agglutinative} & Japanese & 685.0 & 34.25 & 1677.0 & 83.85\\\\ \n",
      "    \\agglutinative{Agglutinative} & Korean & 0.0 & 0.0 & 0.0 & 0.0\\\\ \n",
      "    \\agglutinative{Agglutinative} & Turkish & 184.0 & 100.0 & 184.0 & 100.0\\\\\n",
      "    \\introflexive{Introflexive} & Arabic & 446.0 & 4.43 & 1242.0 & 12.33\\\\\n",
      "    \\introflexive{Introflexive} & Hebrew & 3.0 & 0.17 & 16.0 & 0.9\\\\\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"  \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
