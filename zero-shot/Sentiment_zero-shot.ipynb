{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_sentiment import Example, convert_examples_to_tf_dataset, make_batches\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with:    Korean \n",
      "\n",
      "Already evaluated:  Arabic  Bulgarian  English  Basque  Finnish  Hebrew  Croatian  Slovak  Thai  Vietnamese  Chinese\n",
      "\n",
      "Not yet evaluated:\n",
      "\n",
      "Still to train:   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_dicts = utils.make_lang_code_dicts()\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "results_path = \"../results/results_sentiment.xlsx\"\n",
    "\n",
    "# Look for languages that have sentiment weights but are not in the results file\n",
    "file = open(\"../data_exploration/sentiment_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "all_langs = [lang for lang in all_langs if lang not in [\"Turkish\", \"Japanese\", \"Russian\"]]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/*sentiment.hdf5\")]\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "    remaining_langs = [lang for lang in trained_langs if lang not in results[\"Accuracy\"].columns]\n",
    "else:\n",
    "    remaining_langs = trained_langs\n",
    "    \n",
    "untrained_langs = [lang for lang in all_langs if lang not in trained_langs]\n",
    "evaluated_langs = [lang for lang in trained_langs if lang not in remaining_langs]\n",
    "\n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"Evaluating with:   \", training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet evaluated:\"] + remaining_langs[1:], displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\")\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights from E:/TFM_CCIL/checkpoints/ko/bert-base-multilingual-cased_sentiment.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "max_length = 512\n",
    "batch_size = 64\n",
    "\n",
    "# Model creation and loading weights\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "weights_path = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "weights_filename = model_name + \"_sentiment.hdf5\"\n",
    "model.load_weights(weights_path + weights_filename)\n",
    "print(\"Using weights from\", weights_path + weights_filename)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826f347e756d432bb6571f843483e4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 389s 3s/step\n",
      "27/27 [==============================] - 65s 2s/step\n",
      "29/29 [==============================] - 72s 2s/step\n",
      "4/4 [==============================] - 7s 2s/step\n",
      "7/7 [==============================] - 13s 2s/step\n",
      "40/40 [==============================] - 101s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 15s 2s/step\n",
      "15/15 [==============================] - 35s 2s/step\n",
      "17/17 [==============================] - 41s 2s/step\n",
      "37/37 [==============================] - 92s 2s/step\n",
      "11/11 [==============================] - 25s 2s/step\n",
      "86/86 [==============================] - 221s 3s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "sentiment_eval = []\n",
    "\n",
    "for lang in tqdm(os.listdir(path)):\n",
    "    if lang not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        # Load and preprocess\n",
    "        test = pd.read_csv(path + lang + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "        test = test[lengths <= 512].reset_index(drop=True) # Remove long examples\n",
    "        \n",
    "        # Convert to TF dataset\n",
    "        test_dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                                       text in test.values], \n",
    "                                                      tokenizer, max_length=max_length)\n",
    "        test_dataset, test_batches = make_batches(test_dataset, batch_size, repetitions=1, shuffle=False)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(test_dataset, steps=np.ceil(test.shape[0] / batch_size), verbose=1)\n",
    "        clean_preds = preds[0].argmax(axis=-1)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(test[\"sentiment\"].values, clean_preds)\n",
    "        precision = precision_score(test[\"sentiment\"].values, clean_preds, average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "        f1 = f1_score(test[\"sentiment\"].values, clean_preds, average=\"macro\")\n",
    "        sentiment_eval.append((lang, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table for this training language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_eval = np.array(sentiment_eval, dtype=object)\n",
    "table = pd.DataFrame({\"Language\": sentiment_eval[:,0],\n",
    "                      \"Accuracy\": sentiment_eval[:,1],\n",
    "                      \"Macro_Precision\": sentiment_eval[:,2],\n",
    "                      \"Macro_Recall\": sentiment_eval[:,3],\n",
    "                      \"Macro_F1\": sentiment_eval[:,4]})\n",
    "table[\"Language\"] = table[\"Language\"].apply(lambda x: code_to_name[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder so that language types are grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data_exploration/sentiment_table.txt\", \"r\")\n",
    "lang_order = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "lang_order = [lang for lang in lang_order if lang not in [\"Turkish\", \"Japanese\", \"Russian\"]]\n",
    "table[\"sort\"] = table[\"Language\"].apply(lambda x: lang_order.index(x))\n",
    "table = table.sort_values(by=[\"sort\"]).drop(\"sort\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro_Precision</th>\n",
       "      <th>Macro_Recall</th>\n",
       "      <th>Macro_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>0.552301</td>\n",
       "      <td>0.541514</td>\n",
       "      <td>0.564149</td>\n",
       "      <td>0.502776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>0.667765</td>\n",
       "      <td>0.695968</td>\n",
       "      <td>0.668076</td>\n",
       "      <td>0.655618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Slovak</td>\n",
       "      <td>0.724624</td>\n",
       "      <td>0.624845</td>\n",
       "      <td>0.764238</td>\n",
       "      <td>0.620669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Croatian</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.584019</td>\n",
       "      <td>0.617898</td>\n",
       "      <td>0.509891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.59124</td>\n",
       "      <td>0.635051</td>\n",
       "      <td>0.628417</td>\n",
       "      <td>0.590373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>0.590643</td>\n",
       "      <td>0.600901</td>\n",
       "      <td>0.594133</td>\n",
       "      <td>0.584966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thai</td>\n",
       "      <td>0.513699</td>\n",
       "      <td>0.540079</td>\n",
       "      <td>0.53873</td>\n",
       "      <td>0.513039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.630442</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.613521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Basque</td>\n",
       "      <td>0.625551</td>\n",
       "      <td>0.562279</td>\n",
       "      <td>0.615104</td>\n",
       "      <td>0.535385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Korean</td>\n",
       "      <td>0.75054</td>\n",
       "      <td>0.745549</td>\n",
       "      <td>0.750737</td>\n",
       "      <td>0.746769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>0.703738</td>\n",
       "      <td>0.56679</td>\n",
       "      <td>0.594048</td>\n",
       "      <td>0.569244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hebrew</td>\n",
       "      <td>0.601489</td>\n",
       "      <td>0.409475</td>\n",
       "      <td>0.43327</td>\n",
       "      <td>0.400397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language  Accuracy Macro_Precision Macro_Recall  Macro_F1\n",
       "0    Bulgarian  0.552301        0.541514     0.564149  0.502776\n",
       "1      English  0.667765        0.695968     0.668076  0.655618\n",
       "2       Slovak  0.724624        0.624845     0.764238  0.620669\n",
       "3     Croatian  0.526316        0.584019     0.617898  0.509891\n",
       "4      Chinese   0.59124        0.635051     0.628417  0.590373\n",
       "5   Vietnamese  0.590643        0.600901     0.594133  0.584966\n",
       "6         Thai  0.513699        0.540079      0.53873  0.513039\n",
       "7      Finnish  0.642857        0.630442     0.673469  0.613521\n",
       "8       Basque  0.625551        0.562279     0.615104  0.535385\n",
       "9       Korean   0.75054        0.745549     0.750737  0.746769\n",
       "10      Arabic  0.703738         0.56679     0.594048  0.569244\n",
       "11      Hebrew  0.601489        0.409475      0.43327  0.400397"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update results excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/results_sentiment.xlsx\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "else:\n",
    "    results = dict.fromkeys(table.columns[1:].values, pd.DataFrame({\"Language\": table[\"Language\"].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(results_path) as writer:\n",
    "    full_training_lang = code_to_name[training_lang]\n",
    "    for sheet_name, df in results.items():\n",
    "        # Add each the column for each metric in the corresponding sheet\n",
    "        df[full_training_lang] = table[sheet_name]\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
