{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "from pos_utils import load_data, filter_padding_tokens, find_subword_locations, reconstruct_subwords\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import ABSATokenizer, convert_examples_to_tf_dataset, read_conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def ignore_acc(y_true_class, y_pred_class, class_to_ignore=0):\n",
    "    y_pred_class = K.cast(K.argmax(y_pred_class, axis=-1), 'int32')\n",
    "    y_true_class = K.cast(y_true_class, 'int32')\n",
    "    ignore_mask = K.cast(K.not_equal(y_true_class, class_to_ignore), 'int32')\n",
    "    matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_lang = \"en\"\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tagset = [\"O\", \"_\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \n",
    "          \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "num_labels = len(tagset)\n",
    "label_map = {label: i for i, label in enumerate(tagset)}\n",
    "\n",
    "tokenizer = ABSATokenizer.from_pretrained(model_name)\n",
    "config = transformers.BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name,\n",
    "                                                     config=config)\n",
    "weights_path = \"../checkpoints_\" + training_lang + \"/\"\n",
    "model.load_weights(weights_path + [file for file in os.listdir(weights_path) if \"checkpoint\" not in file][0])\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, metrics=[ignore_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8c91e274e4149bd09a7025f3a44f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/ud/\"\n",
    "pos_eval = {}\n",
    "for directory in tqdm(os.listdir(data_dir)):\n",
    "    path = os.path.join(data_dir, directory)\n",
    "    batch_size = 256 # Doesn't really matter here\n",
    "    test_examples, test_dataset = load_data(path, batch_size, tokenizer, tagset)\n",
    "    preds = model.predict(test_dataset, steps=np.ceil(len(test_examples) / batch_size))\n",
    "    tokens, labels, filtered_preds, logits = filter_padding_tokens(test_examples, preds, label_map, tokenizer)\n",
    "    subword_locations = find_subword_locations(tokens)\n",
    "    new_tokens, new_labels, new_preds = reconstruct_subwords(subword_locations, tokens, labels, filtered_preds, logits)\n",
    "    pos_eval[directory] = (np.array(new_labels) == np.array(new_preds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': 0.7342296756782806,\n",
       " 'bg': 0.8471126939709998,\n",
       " 'en': 0.959243908565687,\n",
       " 'eu': 0.6956593090998605,\n",
       " 'fi': 0.8569173454568416,\n",
       " 'he': 0.5951641672722469,\n",
       " 'hr': 0.8481863149216817,\n",
       " 'ja': 0.4850788182873404,\n",
       " 'ko': 0.5937047756874095,\n",
       " 'ru': 0.8474453686005062,\n",
       " 'sl': 0.8351243475591035,\n",
       " 'th': 0.37430017467640075,\n",
       " 'tr': 0.6696060799443059,\n",
       " 'vi': 0.5837113545955754,\n",
       " 'zh': 0.5734765351389213}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_eval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': '95.92%',\n",
       " 'fi': '85.69%',\n",
       " 'hr': '84.82%',\n",
       " 'ru': '84.74%',\n",
       " 'bg': '84.71%',\n",
       " 'sl': '83.51%',\n",
       " 'ar': '73.42%',\n",
       " 'eu': '69.57%',\n",
       " 'tr': '66.96%',\n",
       " 'he': '59.52%',\n",
       " 'ko': '59.37%',\n",
       " 'vi': '58.37%',\n",
       " 'zh': '57.35%',\n",
       " 'ja': '48.51%',\n",
       " 'th': '37.43%'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: str(round(v * 100, 2)) + \"%\" for k, v in sorted(pos_eval.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/results_pos.xlsx\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the sheet already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = \"results_pos_\" + training_lang\n",
    "\n",
    "if sheet in results:\n",
    "    raise Exception(\"Sheet already exists and would be overwritten, aborting\")\n",
    "else:\n",
    "    results[sheet] = pd.DataFrame({\"Language\": list(pos_eval.keys()), \"Test_acc\": list(pos_eval.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all sheets into excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"../results/results_pos.xlsx\") as writer:\n",
    "    for sheet_name, df in results.items():\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_conll(\"../data/ud/fi/fi_pud-ud-test.conllu\")\n",
    "test_examples = [{\"id\": sent_id, \"tokens\": tokens, \"tags\": tags} for sent_id, tokens, tags in zip(test_data[0], \n",
    "                                                                                                  test_data[1],\n",
    "                                                                                                  test_data[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "test_dataset = convert_examples_to_tf_dataset(examples=test_examples, tokenizer=tokenizer, tagset=tagset, max_length=256)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = test_dataset.as_numpy_iterator().next()\n",
    "\n",
    "for token, label in zip(example_batch[0][\"input_ids\"][0], example_batch[1][0]):\n",
    "    if token == 0:\n",
    "        break\n",
    "    print(\"{:<25}{:<20}\".format(tokenizer.decode(int(token)), tagset[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset, steps=np.ceil(len(test_examples) / batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_preds = []\n",
    "labels = []\n",
    "tokens = []\n",
    "logits = []\n",
    "\n",
    "for i in range(len(test_examples)):\n",
    "    example_tokens, example_labels, _ = tokenizer.subword_tokenize(test_examples[i][\"tokens\"], test_examples[i][\"tags\"])\n",
    "    example_labels = [label_map[label] for label in example_labels]\n",
    "    example_preds = preds[0].argmax(axis=-1)[i, :len(example_labels)]\n",
    "    example_logits = preds[0][i, :len(example_labels)]\n",
    "    filtered_preds.extend(example_preds)\n",
    "    labels.extend(example_labels)\n",
    "    tokens.extend(example_tokens)\n",
    "    logits.extend(example_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(labels) == np.array(filtered_preds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = []\n",
    "iterator = test_dataset.as_numpy_iterator()\n",
    "accuracies = []\n",
    "temp_preds = filtered_preds.copy()\n",
    "\n",
    "for batch in iterator:\n",
    "    batch_labels = batch[1][batch[1] != 0]\n",
    "    flattened.extend(batch_labels)\n",
    "    accuracies.append((np.array(batch[1][batch[1] != 0]) == np.array(temp_preds[:len(batch_labels)])).mean())\n",
    "    temp_preds = temp_preds[len(batch_labels):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(labels) == np.array(flattened)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = None\n",
    "end = None\n",
    "subword_locations = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i].startswith(\"##\") and not(tokens[i-1].startswith(\"##\")):\n",
    "        start = i - 1\n",
    "    if not(tokens[i].startswith(\"##\")) and tokens[i-1].startswith(\"##\"):\n",
    "        end = i\n",
    "        subword_locations.append((start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = []\n",
    "final_most_voted = []\n",
    "final_avg = []\n",
    "final_first = []\n",
    "final_random = []\n",
    "final_max_prob = []\n",
    "final_random_equi = []\n",
    "\n",
    "for start, end in subword_locations:\n",
    "    if len(set(filtered_preds[start:end])) > 1:\n",
    "        print(start, end)\n",
    "        print(\"Tokens:\", tokens[start:end])\n",
    "        print(\"Predictions:\", filtered_preds[start:end])\n",
    "        print(\"Truth:\", labels[start])\n",
    "        truths.append(labels[start])\n",
    "        \n",
    "        most_voted = max(set(filtered_preds[start:end]), key=filtered_preds[start:end].count)\n",
    "        final_most_voted.append(most_voted)\n",
    "        avg = sum(logits[start:end]).argmax()\n",
    "        final_avg.append(avg)\n",
    "        final_first.append(filtered_preds[start])\n",
    "        final_random.append(np.random.choice(filtered_preds[start:end]))\n",
    "        temp = np.array([(M.max(), M.argmax()) for M in logits[start:end]])\n",
    "        final_max_prob.append(temp[temp[:,0].argmax(), 1])\n",
    "        final_random_equi.append(np.random.choice(list(set(filtered_preds[start:end]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most voted:\", (np.array(truths) == np.array(final_most_voted)).mean())\n",
    "print(\"Logit average:\", (np.array(truths) == np.array(final_avg)).mean())\n",
    "print(\"Always first:\", (np.array(truths) == np.array(final_first)).mean())\n",
    "print(\"Random choice:\", (np.array(truths) == np.array(final_random)).mean())\n",
    "print(\"Highest probability:\", (np.array(truths) == np.array(final_max_prob)).mean())\n",
    "print(\"Equiprobable random:\", (np.array(truths) == np.array(final_random_equi)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = []\n",
    "new_preds = []\n",
    "new_labels = []\n",
    "prev_end = 0\n",
    "\n",
    "for start, end in subword_locations:\n",
    "    if len(set(filtered_preds[start:end])) > 1:\n",
    "        # Subword predictions do not all agree\n",
    "        prediction = sum(logits[start:end]).argmax()\n",
    "    else:\n",
    "        prediction = filtered_preds[start]\n",
    "    new_preds += filtered_preds[prev_end:start] + [prediction]\n",
    "    token = \"\".join(tokens[start:end]).replace(\"##\", \"\")\n",
    "    new_tokens += tokens[prev_end:start] + [token]\n",
    "    new_labels += labels[prev_end:start] + [labels[start]]\n",
    "    prev_end = end\n",
    "    \n",
    "# Last subword onwards\n",
    "new_preds += filtered_preds[prev_end:]\n",
    "new_tokens += tokens[prev_end:]\n",
    "new_labels += labels[prev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(new_tokens[:20], new_labels[:20]):\n",
    "    print(token, tagset[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(tokens[:30], labels[:30]):\n",
    "    print(token, tagset[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(new_labels) == np.array(new_preds)).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
