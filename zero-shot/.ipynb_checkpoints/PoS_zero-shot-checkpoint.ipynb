{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertForTokenClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import ABSATokenizer, convert_examples_to_tf_dataset, read_conll\n",
    "import utils.utils as utils\n",
    "import utils.pos_utils as pos_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with:    Bulgarian \n",
      "\n",
      "Already evaluated:  English\n",
      "\n",
      "Not yet evaluated:  Basque  Hebrew  Croatian  Russian  Slovak  Vietnamese\n",
      "\n",
      "Still to train:     Chinese  Thai  Finnish  Japanese  Korean  Turkish  Arabic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_dicts = utils.make_lang_code_dicts(\"../utils/lang_codes.xlsx\")\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "results_path = \"../results/results_pos.xlsx\"\n",
    "\n",
    "# Look for languages that have PoS weights but are not in the results file\n",
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/*pos.hdf5\")]\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "    remaining_langs = [lang for lang in trained_langs if lang not in results[\"Accuracy\"].columns]\n",
    "else:\n",
    "    remaining_langs = trained_langs\n",
    "    \n",
    "untrained_langs = [lang for lang in all_langs if lang not in trained_langs]\n",
    "evaluated_langs = [lang for lang in trained_langs if lang not in remaining_langs]\n",
    "    \n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"Evaluating with:   \", training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet evaluated:\"] + remaining_langs[1:], displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\", \"\\n\")\n",
    "    print(IPython.utils.text.columnize([\"Already evaluated:\"] + evaluated_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Still to train:   \"] + untrained_langs, displaywidth=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights from E:/TFM_CCIL/checkpoints/en/bert-base-multilingual-cased_pos.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "max_length = 256\n",
    "batch_size = 256\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tagset = [\"O\", \"_\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \n",
    "          \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "num_labels = len(tagset)\n",
    "label_map = {label: i for i, label in enumerate(tagset)}\n",
    "\n",
    "# Model creation and loading weights\n",
    "tokenizer = ABSATokenizer.from_pretrained(model_name)\n",
    "config = transformers.BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name,\n",
    "                                                     config=config)\n",
    "weights_path = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "weights_filename = model_name + \"_pos.hdf5\"\n",
    "model.load_weights(weights_path + weights_filename)\n",
    "print(\"Using weights from\", weights_path + weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65cd42c24be471fac6bcc923f41f333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 12s 3s/step\n",
      "5/5 [==============================] - 14s 3s/step\n",
      "4/4 [==============================] - 11s 3s/step\n",
      "8/8 [==============================] - 26s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "2/2 [==============================] - 4s 2s/step\n",
      "5/5 [==============================] - 15s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "5/5 [==============================] - 14s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "4/4 [==============================] - 10s 2s/step\n",
      "4/4 [==============================] - 13s 3s/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/ud/\"\n",
    "pos_eval = []\n",
    "\n",
    "for directory in tqdm(os.listdir(data_dir)):\n",
    "    # Load and preprocess\n",
    "    path = os.path.join(data_dir, directory)\n",
    "    test_examples, test_dataset = pos_utils.load_data(path, batch_size, tokenizer, tagset)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(test_dataset, steps=np.ceil(len(test_examples) / batch_size), verbose=1)\n",
    "    \n",
    "    # Postprocessing\n",
    "    tokens, labels, filtered_preds, logits = pos_utils.filter_padding_tokens(test_examples, preds, label_map, tokenizer)\n",
    "    subword_locations = pos_utils.find_subword_locations(tokens)\n",
    "    new_tokens, new_labels, new_preds = pos_utils.reconstruct_subwords(subword_locations, tokens, labels, \n",
    "                                                                       filtered_preds, logits)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (np.array(new_labels) == np.array(new_preds)).mean()\n",
    "    pos_eval.append((directory, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table for this training language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eval = np.array(pos_eval, dtype=object)\n",
    "table = pd.DataFrame({\"Language\": pos_eval[:,0],\n",
    "                      \"Accuracy\": pos_eval[:,1]})\n",
    "table[\"Language\"] = table[\"Language\"].apply(lambda x: code_to_name[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder so that language types are grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "lang_order = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "table[\"sort\"] = table[\"Language\"].apply(lambda x: lang_order.index(x))\n",
    "table = table.sort_values(by=[\"sort\"]).drop(\"sort\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>0.847113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>0.959244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russian</td>\n",
       "      <td>0.847445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slovak</td>\n",
       "      <td>0.835124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Croatian</td>\n",
       "      <td>0.848186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.573477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>0.583711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thai</td>\n",
       "      <td>0.3743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>0.856917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basque</td>\n",
       "      <td>0.695659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>0.485079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Korean</td>\n",
       "      <td>0.593705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>0.669606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>0.73423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hebrew</td>\n",
       "      <td>0.595164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language  Accuracy\n",
       "0    Bulgarian  0.847113\n",
       "1      English  0.959244\n",
       "2      Russian  0.847445\n",
       "3       Slovak  0.835124\n",
       "4     Croatian  0.848186\n",
       "5      Chinese  0.573477\n",
       "6   Vietnamese  0.583711\n",
       "7         Thai    0.3743\n",
       "8      Finnish  0.856917\n",
       "9       Basque  0.695659\n",
       "10    Japanese  0.485079\n",
       "11      Korean  0.593705\n",
       "12     Turkish  0.669606\n",
       "13      Arabic   0.73423\n",
       "14      Hebrew  0.595164"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update results excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/results_pos.xlsx\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "else:\n",
    "    results = dict.fromkeys(table.columns[1:].values, pd.DataFrame({\"Language\": table[\"Language\"].values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(results_path) as writer:\n",
    "    full_training_lang = code_to_name[training_lang]\n",
    "    for sheet_name, df in results.items():\n",
    "        # Add each the column for each metric in the corresponding sheet\n",
    "        df[full_training_lang] = table[sheet_name]\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8c91e274e4149bd09a7025f3a44f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/ud/\"\n",
    "pos_eval = {}\n",
    "for directory in tqdm(os.listdir(data_dir)):\n",
    "    path = os.path.join(data_dir, directory)\n",
    "    batch_size = 256 # Doesn't really matter here\n",
    "    test_examples, test_dataset = load_data(path, batch_size, tokenizer, tagset)\n",
    "    preds = model.predict(test_dataset, steps=np.ceil(len(test_examples) / batch_size))\n",
    "    tokens, labels, filtered_preds, logits = filter_padding_tokens(test_examples, preds, label_map, tokenizer)\n",
    "    subword_locations = find_subword_locations(tokens)\n",
    "    new_tokens, new_labels, new_preds = reconstruct_subwords(subword_locations, tokens, labels, filtered_preds, logits)\n",
    "    pos_eval[directory] = (np.array(new_labels) == np.array(new_preds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': 0.7342296756782806,\n",
       " 'bg': 0.8471126939709998,\n",
       " 'en': 0.959243908565687,\n",
       " 'eu': 0.6956593090998605,\n",
       " 'fi': 0.8569173454568416,\n",
       " 'he': 0.5951641672722469,\n",
       " 'hr': 0.8481863149216817,\n",
       " 'ja': 0.4850788182873404,\n",
       " 'ko': 0.5937047756874095,\n",
       " 'ru': 0.8474453686005062,\n",
       " 'sl': 0.8351243475591035,\n",
       " 'th': 0.37430017467640075,\n",
       " 'tr': 0.6696060799443059,\n",
       " 'vi': 0.5837113545955754,\n",
       " 'zh': 0.5734765351389213}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_eval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': '95.92%',\n",
       " 'fi': '85.69%',\n",
       " 'hr': '84.82%',\n",
       " 'ru': '84.74%',\n",
       " 'bg': '84.71%',\n",
       " 'sl': '83.51%',\n",
       " 'ar': '73.42%',\n",
       " 'eu': '69.57%',\n",
       " 'tr': '66.96%',\n",
       " 'he': '59.52%',\n",
       " 'ko': '59.37%',\n",
       " 'vi': '58.37%',\n",
       " 'zh': '57.35%',\n",
       " 'ja': '48.51%',\n",
       " 'th': '37.43%'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: str(round(v * 100, 2)) + \"%\" for k, v in sorted(pos_eval.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"../results/results_pos.xlsx\"\n",
    "\n",
    "if os.path.isfile(results_path):\n",
    "    results = pd.read_excel(results_path, sheet_name=None)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the sheet already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = \"results_pos_\" + training_lang\n",
    "\n",
    "if sheet in results:\n",
    "    raise Exception(\"Sheet already exists and would be overwritten, aborting\")\n",
    "else:\n",
    "    results[sheet] = pd.DataFrame({\"Language\": list(pos_eval.keys()), \"Test_acc\": list(pos_eval.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all sheets into excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"../results/results_pos.xlsx\") as writer:\n",
    "    for sheet_name, df in results.items():\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
