{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_preparation_sentiment import Example, convert_examples_to_tf_dataset, make_batches\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
    "model.load_weights(\"../checkpoints/multibert_sentiment_0.883.hdf5\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "max_length = 512\n",
    "batch_size = 128\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/sentiment/hr/test.csv\", header=None)\n",
    "test.columns = [\"sentiment\", \"review\"]\n",
    "lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "test = test[lengths <= 512].reset_index(drop=True) # Remove long examples\n",
    "test_dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                               text in test.values], \n",
    "                                              tokenizer, max_length=max_length)\n",
    "test_dataset, test_batches = make_batches(test_dataset, batch_size, repetitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803203661327232"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"sentiment\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step - loss: 1.1602 - sparse_categorical_accuracy: 0.6071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1601811647415161, 0.6071428656578064]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fc7e8c1f6f4bbeb443aa9149edc64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar 0.8325724493846764\n",
      "bg 0.8111050626020686\n",
      "en 0.49917627677100496\n",
      "eu 0.8458149779735683\n",
      "fi 0.7455919395465995\n",
      "he 0.040654997176736304\n",
      "hr 0.7803203661327232\n",
      "ko 2.8757886435331232\n",
      "sl 0.9219924812030075\n",
      "th 0.40784982935153585\n",
      "vi 0.5138686131386861\n",
      "zh 0.6045710139669871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "\n",
    "for lang in tqdm(os.listdir(path)):\n",
    "    if lang not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        test = pd.read_csv(path + lang + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        print(lang, test[\"sentiment\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c738f9304ef43e18411479d5bcd99b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 376s 5s/step - loss: 0.9112 - sparse_categorical_accuracy: 0.6229\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.9480 - sparse_categorical_accuracy: 0.7153\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.5955 - sparse_categorical_accuracy: 0.8638\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.6908 - sparse_categorical_accuracy: 0.4361\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.5446 - sparse_categorical_accuracy: 0.4719\n",
      "14/14 [==============================] - 66s 5s/step - loss: 4.1296 - sparse_categorical_accuracy: 0.1974\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.3993 - sparse_categorical_accuracy: 0.5217\n",
      "40/40 [==============================] - 198s 5s/step - loss: nan - sparse_categorical_accuracy: 0.1232\n",
      "9/9 [==============================] - 37s 4s/step - loss: 1.0524 - sparse_categorical_accuracy: 0.6786\n",
      "19/19 [==============================] - 88s 5s/step - loss: 0.9419 - sparse_categorical_accuracy: 0.6289\n",
      "6/6 [==============================] - 22s 4s/step - loss: 1.3330 - sparse_categorical_accuracy: 0.6199\n",
      "43/43 [==============================] - 214s 5s/step - loss: 1.3811 - sparse_categorical_accuracy: 0.5985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "sentiment_eval = {}\n",
    "\n",
    "for lang in tqdm(os.listdir(path)):\n",
    "    if lang not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        test = pd.read_csv(path + lang + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "        test = test[lengths <= 512] # Remove long examples\n",
    "        test_dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                                       text in test.values], \n",
    "                                                      tokenizer, max_length=max_length)\n",
    "        test_dataset, test_batches = make_batches(test_dataset, batch_size, repetitions=1)\n",
    "        sentiment_eval[lang] = model.evaluate(test_dataset, steps=test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': 0.6229491233825684,\n",
       " 'bg': 0.7152966856956482,\n",
       " 'en': 0.8638110756874084,\n",
       " 'eu': 0.4361233413219452,\n",
       " 'fi': 0.4719387888908386,\n",
       " 'he': 0.19739818572998047,\n",
       " 'hr': 0.52173912525177,\n",
       " 'ko': 0.12322555482387543,\n",
       " 'sl': 0.6785714030265808,\n",
       " 'th': 0.6288527250289917,\n",
       " 'vi': 0.6198830604553223,\n",
       " 'zh': 0.5985096096992493}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': '86.38%',\n",
       " 'bg': '71.53%',\n",
       " 'sl': '67.86%',\n",
       " 'th': '62.89%',\n",
       " 'ar': '62.29%',\n",
       " 'vi': '61.99%',\n",
       " 'zh': '59.85%',\n",
       " 'hr': '52.17%',\n",
       " 'fi': '47.19%',\n",
       " 'eu': '43.61%',\n",
       " 'he': '19.74%',\n",
       " 'ko': '12.32%'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: str(round(v * 100, 2)) + \"%\" for k, v in sorted(sentiment_eval.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba89ef8b1d9048ff9511934783c5eaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/sentiment/\"\n",
    "max_lengths = {}\n",
    "for directory in tqdm(os.listdir(path)):\n",
    "    if directory not in [\"tr\", \"ja\", \"ru\"]:\n",
    "        lang_path = os.path.join(path, directory)\n",
    "        test = pd.read_csv(lang_path + \"/test.csv\", header=None)\n",
    "        test.columns = [\"sentiment\", \"review\"]\n",
    "        lengths = test[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "        max_lengths[directory] = (lengths.max(), (lengths > 512).sum(), (lengths > 256).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language       Length of           Number of examples  Number of examples  \n",
      "               longest example     above 512 tokens    above 256 tokens    \n",
      "\n",
      "ar             5040                446                 1242      \n",
      "bg             65                  0                   0         \n",
      "en             77                  0                   0         \n",
      "eu             79                  0                   0         \n",
      "fi             657                 5                   26        \n",
      "he             1183                3                   16        \n",
      "hr             316                 0                   1         \n",
      "ko             212                 0                   0         \n",
      "sl             411                 0                   9         \n",
      "th             1336                8                   48        \n",
      "vi             826                 1                   3         \n",
      "zh             847                 11                  106       \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15}{:<20}{:<20}{:<20}\".format(\"Language\", \"Length of\", \"Number of examples\", \"Number of examples\"))\n",
    "print(\"{:<15}{:<20}{:<20}{:<20}\".format(\"\", \"longest example\", \"above 512 tokens\", \"above 256 tokens\") + \"\\n\")\n",
    "for lang, values in max_lengths.items():\n",
    "    print(\"{:<15}{:<20}{:<20}{:<10}\".format(lang, values[0], values[1], values[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "output = \"\"\n",
    "lang_codes = pd.read_excel(\"../data_exploration/lang_codes.xlsx\", header=0)\n",
    "max_lengths[\"sk\"] = max_lengths[\"sl\"]\n",
    "\n",
    "for line in file.readlines():\n",
    "    lang_name = line.split(\"&\")[1].strip()\n",
    "    lang_code = lang_codes[\"ISO 639-1 Code\"][lang_codes[\"English name of Language\"] == lang_name].values[0]\n",
    "    \n",
    "    if lang_code in max_lengths:\n",
    "        values = max_lengths[lang_code]\n",
    "        split_line = line.split(\"\\\\\")\n",
    "        start = split_line[0] + \"\\\\\" + \"&\".join(split_line[1].split(\"&\")[:2])\n",
    "        end = r\"\\\\\" + \"\".join(split_line[2:])\n",
    "        new_line = start + \"& \" + \" & \".join(np.array(values[1:]).astype(str)) + end\n",
    "        \n",
    "    else:\n",
    "        new_line = line\n",
    "        \n",
    "    output += new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \\fusional{Fusional}  & Bulgarian & 0 & 0\\\\ \n",
      "    \\fusional{Fusional} & English & 0 & 0\\\\\n",
      "    \\fusional{Fusional}  & Russian &  &  \\\\ \n",
      "    \\fusional{Fusional} & Slovak & 0 & 9\\\\\n",
      "    \\fusional{Fusional}  & Croatian & 0 & 1\\\\\n",
      "    \\isolating{Isolating} & Chinese & 11 & 106\\\\ \n",
      "    \\isolating{Isolating} & Vietnamese  & 1 & 3\\\\\n",
      "    \\isolating{Isolating} & Thai & 8 & 48\\\\\n",
      "    \\agglutinative{Agglutinative} & Finnish & 5 & 26\\\\ \n",
      "    \\agglutinative{Agglutinative} & Basque & 0 & 0\\\\\n",
      "    \\agglutinative{Agglutinative} & Japanese & \\\\ \n",
      "    \\agglutinative{Agglutinative} & Korean & 0 & 0\\\\ \n",
      "    \\agglutinative{Agglutinative} & Turkish & \\\\\n",
      "    \\introflexive{Introflexive} & Arabic & 446 & 1242\\\\\n",
      "    \\introflexive{Introflexive} & Hebrew & 3 & 16\\\\\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
