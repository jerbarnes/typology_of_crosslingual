{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFBertForTokenClassification, TFXLMRobertaForTokenClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import MBERT_Tokenizer, XLMR_Tokenizer, convert_examples_to_tf_dataset, read_conll\n",
    "import utils.utils as utils\n",
    "from utils.pos_utils import ignore_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language:   Hebrew \n",
      "\n",
      "Already trained:     Arabic  Bulgarian  English  Basque  Finnish  Croatian  Japanese  Korean  Russian  Slovak  Turkish  Vietnamese  Chinese\n",
      "\n",
      "Not yet trained:   \n",
      "\n",
      "Cannot train:        Thai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/ud/\"\n",
    "full_model_name = \"jplu/tf-xlm-roberta-base\"\n",
    "#full_model_name = \"bert-base-multilingual-cased\"\n",
    "model_name = full_model_name.split(\"/\")[-1] # The \"jplu/\" part is problematic\n",
    "\n",
    "code_dicts = utils.make_lang_code_dicts()\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/{}_pos.hdf5\".format(model_name))]\n",
    "cannot_train_langs = []\n",
    "remaining_langs = []\n",
    "for lang in all_langs:\n",
    "    # Check if there are train and dev sets available\n",
    "    if glob.glob(path + name_to_code[lang] + \"/*train.conllu\") and glob.glob(path + name_to_code[lang] + \"/*dev.conllu\"):\n",
    "        if lang not in trained_langs:\n",
    "            remaining_langs.append(lang)\n",
    "    else:\n",
    "        cannot_train_langs.append(lang)\n",
    "\n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"{:<20}\".format(\"Training language:\"), training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already trained:   \"] + trained_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet trained:   \"] + remaining_langs[1:], displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Cannot train:      \"] + cannot_train_langs, displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\", \"\\n\")\n",
    "    print(IPython.utils.text.columnize([\"Cannot train:      \"] + cannot_train_langs, displaywidth=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jplu/tf-xlm-roberta-base were not used when initializing TFXLMRobertaForTokenClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFXLMRobertaForTokenClassification were not initialized from the model checkpoint at jplu/tf-xlm-roberta-base and are newly initialized: ['dropout_38', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "max_length = 256\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "epochs = 20\n",
    "tagset = [\"O\", \"_\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \n",
    "          \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "num_labels = len(tagset)\n",
    "\n",
    "# Model creation\n",
    "if model_name.startswith(\"bert\"):\n",
    "    tokenizer = MBERT_Tokenizer.from_pretrained(full_model_name)\n",
    "    config = transformers.BertConfig.from_pretrained(full_model_name, num_labels=num_labels)\n",
    "    model = TFBertForTokenClassification.from_pretrained(full_model_name,\n",
    "                                                         config=config)\n",
    "else:\n",
    "    tokenizer = XLMR_Tokenizer.from_pretrained(full_model_name)\n",
    "    model = TFXLMRobertaForTokenClassification.from_pretrained(full_model_name, num_labels=num_labels)\n",
    "\n",
    "# Checkpoint for best model weights\n",
    "checkpoint_dir = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint = ModelCheckpoint(checkpoint_dir + model_name + \"_pos_checkpoint.hdf5\", \n",
    "                             verbose=1, monitor=\"val_ignore_acc\",\n",
    "                             save_best_only=True, mode=\"max\", save_weights_only=True)\n",
    "\n",
    "# Model compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[ignore_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "פ                        NOUN                \n",
      "דר                       NOUN                \n",
      "ציה                      NOUN                \n",
      "אחרת                     ADJ                 \n",
      "העו                      _                   \n",
      "מדת                      _                   \n",
      "ה                        SCONJ               \n",
      "עומד                     VERB                \n",
      "ת                        VERB                \n",
      "להת                      VERB                \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for dataset_name in [\"train\", \"dev\"]:\n",
    "    # look for the data file\n",
    "    try:\n",
    "        file_path = glob.glob(path + training_lang + \"/*-{}.conllu\".format(dataset_name))[0]\n",
    "    except IndexError:\n",
    "        raise Exception(\"Could not find \" + dataset_name + \" data file\")\n",
    "        \n",
    "    # Load and extract info\n",
    "    conllu_data = read_conll(file_path)\n",
    "    examples = [{\"id\": sent_id, \"tokens\": tokens, \"tags\": tags} for sent_id, tokens, tags in zip(conllu_data[0], \n",
    "                                                                                                 conllu_data[1],\n",
    "                                                                                                 conllu_data[2])]\n",
    "    # In case some example is over max length\n",
    "    examples = [example for example in examples if len(tokenizer.subword_tokenize(example[\"tokens\"], \n",
    "                                                                                  example[\"tags\"])[0]) <= max_length]\n",
    "    \n",
    "    # Convert to TF dataset\n",
    "    dataset = convert_examples_to_tf_dataset(examples=examples, tokenizer=tokenizer, tagset=tagset, max_length=max_length)\n",
    "    if dataset_name == \"train\":\n",
    "        dataset = dataset.shuffle(100000, reshuffle_each_iteration=True).batch(batch_size).repeat(epochs)\n",
    "    else:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "    datasets[dataset_name] = (examples, dataset)\n",
    "    \n",
    "train_examples, train_dataset = datasets[\"train\"]\n",
    "dev_examples, dev_dataset = datasets[\"dev\"]\n",
    "\n",
    "# Print an example sentence for sanity\n",
    "example_batch = train_dataset.as_numpy_iterator().next()\n",
    "for token, label in zip(example_batch[0][\"input_ids\"][0], example_batch[1][0]):\n",
    "    if not token:\n",
    "        break\n",
    "    elif token == example_batch[0][\"input_ids\"][0][10]:\n",
    "        print(\"...\")\n",
    "        break\n",
    "    print(\"{:<25}{:<20}\".format(tokenizer.decode(int(token)), tagset[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(checkpoint_dir + model_name + \"_pos_checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_for_token_classification/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_for_token_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.2835 - ignore_acc: 0.7016\n",
      "Epoch 00001: val_ignore_acc improved from -inf to 0.93807, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 424s 646ms/step - loss: 0.2835 - ignore_acc: 0.7016 - val_loss: 0.0417 - val_ignore_acc: 0.9381\n",
      "Epoch 2/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0399 - ignore_acc: 0.9515\n",
      "Epoch 00002: val_ignore_acc improved from 0.93807 to 0.95742, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 428s 653ms/step - loss: 0.0399 - ignore_acc: 0.9515 - val_loss: 0.0270 - val_ignore_acc: 0.9574\n",
      "Epoch 3/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0277 - ignore_acc: 0.9640\n",
      "Epoch 00003: val_ignore_acc improved from 0.95742 to 0.96128, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 428s 652ms/step - loss: 0.0277 - ignore_acc: 0.9640 - val_loss: 0.0237 - val_ignore_acc: 0.9613\n",
      "Epoch 4/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0242 - ignore_acc: 0.9687\n",
      "Epoch 00004: val_ignore_acc improved from 0.96128 to 0.96267, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 425s 647ms/step - loss: 0.0242 - ignore_acc: 0.9687 - val_loss: 0.0229 - val_ignore_acc: 0.9627\n",
      "Epoch 5/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0187 - ignore_acc: 0.9729\n",
      "Epoch 00005: val_ignore_acc improved from 0.96267 to 0.96349, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 425s 648ms/step - loss: 0.0187 - ignore_acc: 0.9729 - val_loss: 0.0228 - val_ignore_acc: 0.9635\n",
      "Epoch 6/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0158 - ignore_acc: 0.9774\n",
      "Epoch 00006: val_ignore_acc improved from 0.96349 to 0.96696, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 427s 651ms/step - loss: 0.0158 - ignore_acc: 0.9774 - val_loss: 0.0208 - val_ignore_acc: 0.9670\n",
      "Epoch 7/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0120 - ignore_acc: 0.9832\n",
      "Epoch 00007: val_ignore_acc improved from 0.96696 to 0.97197, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 426s 649ms/step - loss: 0.0120 - ignore_acc: 0.9832 - val_loss: 0.0202 - val_ignore_acc: 0.9720\n",
      "Epoch 8/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0099 - ignore_acc: 0.9862\n",
      "Epoch 00008: val_ignore_acc improved from 0.97197 to 0.97379, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 423s 645ms/step - loss: 0.0099 - ignore_acc: 0.9862 - val_loss: 0.0195 - val_ignore_acc: 0.9738\n",
      "Epoch 9/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0085 - ignore_acc: 0.9878\n",
      "Epoch 00009: val_ignore_acc did not improve from 0.97379\n",
      "656/656 [==============================] - 416s 634ms/step - loss: 0.0085 - ignore_acc: 0.9878 - val_loss: 0.0201 - val_ignore_acc: 0.9726\n",
      "Epoch 10/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0074 - ignore_acc: 0.9898\n",
      "Epoch 00010: val_ignore_acc improved from 0.97379 to 0.97424, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0074 - ignore_acc: 0.9898 - val_loss: 0.0209 - val_ignore_acc: 0.9742\n",
      "Epoch 11/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0069 - ignore_acc: 0.9904\n",
      "Epoch 00011: val_ignore_acc improved from 0.97424 to 0.97461, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 424s 646ms/step - loss: 0.0069 - ignore_acc: 0.9904 - val_loss: 0.0206 - val_ignore_acc: 0.9746\n",
      "Epoch 12/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0063 - ignore_acc: 0.9911\n",
      "Epoch 00012: val_ignore_acc improved from 0.97461 to 0.97538, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 427s 650ms/step - loss: 0.0063 - ignore_acc: 0.9911 - val_loss: 0.0197 - val_ignore_acc: 0.9754\n",
      "Epoch 13/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0052 - ignore_acc: 0.9927\n",
      "Epoch 00013: val_ignore_acc improved from 0.97538 to 0.97708, saving model to E:/TFM_CCIL/checkpoints/he/tf-xlm-roberta-base_pos_checkpoint.hdf5\n",
      "656/656 [==============================] - 426s 650ms/step - loss: 0.0052 - ignore_acc: 0.9927 - val_loss: 0.0197 - val_ignore_acc: 0.9771\n",
      "Epoch 14/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0049 - ignore_acc: 0.9930\n",
      "Epoch 00014: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 419s 639ms/step - loss: 0.0049 - ignore_acc: 0.9930 - val_loss: 0.0218 - val_ignore_acc: 0.9741\n",
      "Epoch 15/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0052 - ignore_acc: 0.9928\n",
      "Epoch 00015: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0052 - ignore_acc: 0.9928 - val_loss: 0.0204 - val_ignore_acc: 0.9757\n",
      "Epoch 16/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0043 - ignore_acc: 0.9940\n",
      "Epoch 00016: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 641ms/step - loss: 0.0043 - ignore_acc: 0.9940 - val_loss: 0.0219 - val_ignore_acc: 0.9746\n",
      "Epoch 17/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0053 - ignore_acc: 0.9928\n",
      "Epoch 00017: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 421s 642ms/step - loss: 0.0053 - ignore_acc: 0.9928 - val_loss: 0.0221 - val_ignore_acc: 0.9749\n",
      "Epoch 18/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0031 - ignore_acc: 0.9956\n",
      "Epoch 00018: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 422s 644ms/step - loss: 0.0031 - ignore_acc: 0.9956 - val_loss: 0.0255 - val_ignore_acc: 0.9745\n",
      "Epoch 19/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0027 - ignore_acc: 0.9961\n",
      "Epoch 00019: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 420s 640ms/step - loss: 0.0027 - ignore_acc: 0.9961 - val_loss: 0.0225 - val_ignore_acc: 0.9756\n",
      "Epoch 20/20\n",
      "656/656 [==============================] - ETA: 0s - loss: 0.0028 - ignore_acc: 0.9960\n",
      "Epoch 00020: val_ignore_acc did not improve from 0.97708\n",
      "656/656 [==============================] - 420s 640ms/step - loss: 0.0028 - ignore_acc: 0.9960 - val_loss: 0.0243 - val_ignore_acc: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xc6b49c74e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=epochs, steps_per_epoch=np.ceil(len(train_examples) / batch_size),\n",
    "          validation_data=dev_dataset, validation_steps=np.ceil(len(dev_examples) / batch_size),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
