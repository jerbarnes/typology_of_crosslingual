{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import ABSATokenizer, convert_examples_to_tf_dataset, read_conll\n",
    "import utils.utils as utils\n",
    "from utils.pos_utils import ignore_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No languages remaining \n",
      "\n",
      "Cannot train:        Thai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/ud/\"\n",
    "\n",
    "code_dicts = utils.make_lang_code_dicts(\"../utils/lang_codes.xlsx\")\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "\n",
    "file = open(\"../data_exploration/pos_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/*pos.hdf5\")]\n",
    "cannot_train_langs = []\n",
    "remaining_langs = []\n",
    "for lang in all_langs:\n",
    "    # Check if there are train and dev sets available\n",
    "    if glob.glob(path + name_to_code[lang] + \"/*train.conllu\") and glob.glob(path + name_to_code[lang] + \"/*dev.conllu\"):\n",
    "        if lang not in trained_langs:\n",
    "            remaining_langs.append(lang)\n",
    "    else:\n",
    "        cannot_train_langs.append(lang)\n",
    "\n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"{:<20}\".format(\"Training language:\"), training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already trained:   \"] + trained_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet trained:   \"] + remaining_langs[1:], displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Cannot train:      \"] + cannot_train_langs, displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\", \"\\n\")\n",
    "    print(IPython.utils.text.columnize([\"Cannot train:      \"] + cannot_train_langs, displaywidth=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForTokenClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "max_length = 256\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "epochs = 20\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tagset = [\"O\", \"_\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \n",
    "          \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "num_labels = len(tagset)\n",
    "\n",
    "# Model creation\n",
    "tokenizer = ABSATokenizer.from_pretrained(model_name)\n",
    "config = transformers.BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name,\n",
    "                                                     config=config)\n",
    "\n",
    "# Checkpoint for best model weights\n",
    "checkpoint_dir = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint = ModelCheckpoint(checkpoint_dir + model_name + \"_pos_checkpoint.hdf5\", \n",
    "                             verbose=1, monitor=\"val_ignore_acc\",\n",
    "                             save_best_only=True, mode=\"max\", save_weights_only=True)\n",
    "\n",
    "# Model compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[ignore_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K a p                    NOUN                \n",
      "# # ı                    NOUN                \n",
      "d                        NOUN                \n",
      "# # ı ş                  NOUN                \n",
      "# # a r ı                NOUN                \n",
      "# # d a n                NOUN                \n",
      "v u                      VERB                \n",
      "# # r u l                VERB                \n",
      "# # u y                  VERB                \n",
      "# # o r                  VERB                \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for dataset_name in [\"train\", \"dev\"]:\n",
    "    # look for the data file\n",
    "    try:\n",
    "        file_path = glob.glob(path + training_lang + \"/*-{}.conllu\".format(dataset_name))[0]\n",
    "    except IndexError:\n",
    "        raise Exception(\"Could not find \" + dataset_name + \" data file\")\n",
    "        \n",
    "    # Load and extract info\n",
    "    conllu_data = read_conll(file_path)\n",
    "    examples = [{\"id\": sent_id, \"tokens\": tokens, \"tags\": tags} for sent_id, tokens, tags in zip(conllu_data[0], \n",
    "                                                                                                 conllu_data[1],\n",
    "                                                                                                 conllu_data[2])]\n",
    "    # In case some example is over max length\n",
    "    examples = [example for example in examples if len(tokenizer.subword_tokenize(example[\"tokens\"], \n",
    "                                                                                  example[\"tags\"])[0]) <= max_length]\n",
    "    \n",
    "    # Convert to TF dataset\n",
    "    dataset = convert_examples_to_tf_dataset(examples=examples, tokenizer=tokenizer, tagset=tagset, max_length=max_length)\n",
    "    if dataset_name == \"train\":\n",
    "        dataset = dataset.shuffle(100000, reshuffle_each_iteration=True).batch(batch_size).repeat(epochs)\n",
    "    else:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "    datasets[dataset_name] = (examples, dataset)\n",
    "    \n",
    "train_examples, train_dataset = datasets[\"train\"]\n",
    "dev_examples, dev_dataset = datasets[\"dev\"]\n",
    "\n",
    "# Print an example sentence for sanity\n",
    "example_batch = train_dataset.as_numpy_iterator().next()\n",
    "for token, label in zip(example_batch[0][\"input_ids\"][0], example_batch[1][0]):\n",
    "    if not token:\n",
    "        break\n",
    "    elif token == example_batch[0][\"input_ids\"][0][10]:\n",
    "        print(\"...\")\n",
    "        break\n",
    "    print(\"{:<25}{:<20}\".format(tokenizer.decode(int(token)), tagset[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_token_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_token_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1626 - ignore_acc: 0.8581\n",
      "Epoch 00001: val_ignore_acc improved from -inf to 0.95012, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 429s 573ms/step - loss: 0.1626 - ignore_acc: 0.8581 - val_loss: 0.0469 - val_ignore_acc: 0.9501\n",
      "Epoch 2/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0468 - ignore_acc: 0.9565\n",
      "Epoch 00002: val_ignore_acc improved from 0.95012 to 0.95754, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 455s 608ms/step - loss: 0.0468 - ignore_acc: 0.9565 - val_loss: 0.0394 - val_ignore_acc: 0.9575\n",
      "Epoch 3/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0340 - ignore_acc: 0.9671\n",
      "Epoch 00003: val_ignore_acc improved from 0.95754 to 0.95812, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 454s 606ms/step - loss: 0.0340 - ignore_acc: 0.9671 - val_loss: 0.0384 - val_ignore_acc: 0.9581\n",
      "Epoch 4/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0263 - ignore_acc: 0.9738\n",
      "Epoch 00004: val_ignore_acc improved from 0.95812 to 0.96214, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 458s 612ms/step - loss: 0.0263 - ignore_acc: 0.9738 - val_loss: 0.0364 - val_ignore_acc: 0.9621\n",
      "Epoch 5/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0212 - ignore_acc: 0.9788\n",
      "Epoch 00005: val_ignore_acc did not improve from 0.96214\n",
      "748/748 [==============================] - 454s 607ms/step - loss: 0.0212 - ignore_acc: 0.9788 - val_loss: 0.0370 - val_ignore_acc: 0.9616\n",
      "Epoch 6/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0171 - ignore_acc: 0.9825\n",
      "Epoch 00006: val_ignore_acc improved from 0.96214 to 0.96280, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 459s 614ms/step - loss: 0.0171 - ignore_acc: 0.9825 - val_loss: 0.0390 - val_ignore_acc: 0.9628\n",
      "Epoch 7/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0141 - ignore_acc: 0.9855\n",
      "Epoch 00007: val_ignore_acc improved from 0.96280 to 0.96367, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 460s 615ms/step - loss: 0.0141 - ignore_acc: 0.9855 - val_loss: 0.0375 - val_ignore_acc: 0.9637\n",
      "Epoch 8/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0110 - ignore_acc: 0.9883\n",
      "Epoch 00008: val_ignore_acc did not improve from 0.96367\n",
      "748/748 [==============================] - 457s 611ms/step - loss: 0.0110 - ignore_acc: 0.9883 - val_loss: 0.0418 - val_ignore_acc: 0.9609\n",
      "Epoch 9/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0092 - ignore_acc: 0.9903\n",
      "Epoch 00009: val_ignore_acc improved from 0.96367 to 0.96388, saving model to E:/TFM_CCIL/checkpoints/ar/bert-base-multilingual-cased_pos_checkpoint.hdf5\n",
      "748/748 [==============================] - 459s 614ms/step - loss: 0.0092 - ignore_acc: 0.9903 - val_loss: 0.0429 - val_ignore_acc: 0.9639\n",
      "Epoch 10/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0078 - ignore_acc: 0.9920\n",
      "Epoch 00010: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 451s 603ms/step - loss: 0.0078 - ignore_acc: 0.9920 - val_loss: 0.0460 - val_ignore_acc: 0.9617\n",
      "Epoch 11/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0070 - ignore_acc: 0.9925\n",
      "Epoch 00011: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 437s 584ms/step - loss: 0.0070 - ignore_acc: 0.9925 - val_loss: 0.0481 - val_ignore_acc: 0.9613\n",
      "Epoch 12/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0057 - ignore_acc: 0.9938\n",
      "Epoch 00012: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 436s 582ms/step - loss: 0.0057 - ignore_acc: 0.9938 - val_loss: 0.0490 - val_ignore_acc: 0.9633\n",
      "Epoch 13/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0054 - ignore_acc: 0.9941\n",
      "Epoch 00013: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0054 - ignore_acc: 0.9941 - val_loss: 0.0504 - val_ignore_acc: 0.9606\n",
      "Epoch 14/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0047 - ignore_acc: 0.9952\n",
      "Epoch 00014: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0047 - ignore_acc: 0.9952 - val_loss: 0.0545 - val_ignore_acc: 0.9610\n",
      "Epoch 15/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0045 - ignore_acc: 0.9951\n",
      "Epoch 00015: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0045 - ignore_acc: 0.9951 - val_loss: 0.0545 - val_ignore_acc: 0.9611\n",
      "Epoch 16/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0043 - ignore_acc: 0.9956\n",
      "Epoch 00016: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0043 - ignore_acc: 0.9956 - val_loss: 0.0539 - val_ignore_acc: 0.9620\n",
      "Epoch 17/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0036 - ignore_acc: 0.9961\n",
      "Epoch 00017: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0036 - ignore_acc: 0.9961 - val_loss: 0.0526 - val_ignore_acc: 0.9623\n",
      "Epoch 18/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0034 - ignore_acc: 0.9963\n",
      "Epoch 00018: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0034 - ignore_acc: 0.9963 - val_loss: 0.0565 - val_ignore_acc: 0.9627\n",
      "Epoch 19/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0035 - ignore_acc: 0.9963\n",
      "Epoch 00019: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0035 - ignore_acc: 0.9963 - val_loss: 0.0523 - val_ignore_acc: 0.9625\n",
      "Epoch 20/20\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.0028 - ignore_acc: 0.9970\n",
      "Epoch 00020: val_ignore_acc did not improve from 0.96388\n",
      "748/748 [==============================] - 433s 579ms/step - loss: 0.0028 - ignore_acc: 0.9970 - val_loss: 0.0571 - val_ignore_acc: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x601c586908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=epochs, steps_per_epoch=np.ceil(len(train_examples) / batch_size),\n",
    "          validation_data=dev_dataset, validation_steps=np.ceil(len(dev_examples) / batch_size),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
