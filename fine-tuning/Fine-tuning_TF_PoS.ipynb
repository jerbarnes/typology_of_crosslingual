{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_pos import ABSATokenizer, convert_examples_to_tf_dataset, read_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tagset = [\"O\", \"_\", \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \n",
    "          \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "num_labels = len(tagset)\n",
    "max_length = 256\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = ABSATokenizer.from_pretrained(model_name)\n",
    "config = transformers.BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFBertForTokenClassification.from_pretrained(model_name,\n",
    "                                                     config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_lang = \"he\"\n",
    "path = \"../data/ud/\"\n",
    "\n",
    "train_data = read_conll(glob.glob(path + training_lang + \"/*-train.conllu\")[0])\n",
    "train_examples = [{\"id\": sent_id, \"tokens\": tokens, \"tags\": tags} for sent_id, tokens, tags in zip(train_data[0], \n",
    "                                                                                                   train_data[1],\n",
    "                                                                                                   train_data[2])]\n",
    "# In case some example is over max length\n",
    "train_examples = [example for example in train_examples if len(tokenizer.subword_tokenize(example[\"tokens\"], \n",
    "                                                                                          example[\"tags\"])[0]) <= max_length]\n",
    "\n",
    "\n",
    "dev_data = read_conll(glob.glob(path + training_lang + \"/*-dev.conllu\")[0])\n",
    "dev_examples = [{\"id\": sent_id, \"tokens\": tokens, \"tags\": tags} for sent_id, tokens, tags in zip(dev_data[0], \n",
    "                                                                                                 dev_data[1],\n",
    "                                                                                                 dev_data[2])]\n",
    "# In case some example is over max length\n",
    "dev_examples = [example for example in dev_examples if len(tokenizer.subword_tokenize(example[\"tokens\"], \n",
    "                                                                                      example[\"tags\"])[0]) <= max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 30\n",
    "train_dataset = convert_examples_to_tf_dataset(examples=train_examples, tokenizer=tokenizer, \n",
    "                                               tagset=tagset, max_length=max_length)\n",
    "train_dataset = train_dataset.shuffle(100000).batch(batch_size).repeat(epochs)\n",
    "dev_dataset = convert_examples_to_tf_dataset(examples=dev_examples, tokenizer=tokenizer, \n",
    "                                             tagset=tagset, max_length=max_length)\n",
    "dev_dataset = dev_dataset.shuffle(100000).batch(batch_size).repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ה                        _                   \n",
      "# # מ ס פ ר              _                   \n",
      "ה                        DET                 \n",
      "מ ס פ ר                  NOUN                \n",
      ",                        PUNCT               \n",
      "נ                        VERB                \n",
      "# # ר ד                  VERB                \n",
      "# # ף                    VERB                \n",
      "ע ל                      ADP                 \n",
      "-                        PUNCT               \n",
      "י ד י                    NOUN                \n",
      "ז                        NOUN                \n",
      "# # י                    NOUN                \n",
      "# # כ ר                  NOUN                \n",
      "# # ו נ ו ת              NOUN                \n",
      "מ                        ADJ                 \n",
      "# # ר י ם                ADJ                 \n",
      "מ                        _                   \n",
      "# # מ ל                  _                   \n",
      "# # ח מ ת                _                   \n",
      "מ                        ADP                 \n",
      "מ ל ח מ ת                NOUN                \n",
      "ה ע ו ל ם                _                   \n",
      "ה                        DET                 \n",
      "ע ו ל ם                  NOUN                \n",
      "ה ש נ י י ה              _                   \n",
      "ה                        DET                 \n",
      "ש נ י                    NUM                 \n",
      "# # י ה                  NUM                 \n",
      "ו                        _                   \n",
      "# # מ ה                  _                   \n",
      "# # ת                    _                   \n",
      "# # ב                    _                   \n",
      "# # ג ר ו ת              _                   \n",
      "# # ו                    _                   \n",
      "ו                        CCONJ               \n",
      "מ                        ADP                 \n",
      "ה                        NOUN                \n",
      "# # ת                    NOUN                \n",
      "# # ב                    NOUN                \n",
      "# # ג ר ו ת              NOUN                \n",
      "# # _                    NOUN                \n",
      "_                        ADP                 \n",
      "# # ש ל                  ADP                 \n",
      "# # _                    ADP                 \n",
      "_                        PRON                \n",
      "# # ה ו                  PRON                \n",
      "# # א                    PRON                \n",
      "ב                        _                   \n",
      "# # מ ש                  _                   \n",
      "# # ט ר                  _                   \n",
      "ב                        ADP                 \n",
      "מ                        NOUN                \n",
      "# # ש                    NOUN                \n",
      "# # ט ר                  NOUN                \n",
      "ק ו                      ADJ                 \n",
      "# # מ ו                  ADJ                 \n",
      "# # נ י ס                ADJ                 \n",
      "# # ט י                  ADJ                 \n",
      ",                        PUNCT               \n",
      "מ                        VERB                \n",
      "# # ו                    VERB                \n",
      "# # צ                    VERB                \n",
      "# # ג                    VERB                \n",
      "כ                        _                   \n",
      "# # א ד                  _                   \n",
      "# # ם                    _                   \n",
      "כ                        ADP                 \n",
      "א ד ם                    NOUN                \n",
      "ח                        ADJ                 \n",
      "# # ס ר                  ADJ                 \n",
      "א ש                      NOUN                \n",
      "# # ל י ו ת              NOUN                \n",
      "ה                        _                   \n",
      "# # ח                    _                   \n",
      "# # ש                    _                   \n",
      "ה                        SCONJ               \n",
      "ח                        VERB                \n",
      "# # ש                    VERB                \n",
      "ע צ מ ו                  PRON                \n",
      "מ                        VERB                \n",
      "# # נ ו ת                VERB                \n",
      "# # ק                    VERB                \n",
      "מ ה                      _                   \n",
      "# # ח ב ר ה              _                   \n",
      "מ                        ADP                 \n",
      "ה                        DET                 \n",
      "ח ב ר ה                  NOUN                \n",
      "ה                        _                   \n",
      "# # א                    _                   \n",
      "# # נ ו                  _                   \n",
      "# # ש י ת                _                   \n",
      "ה                        DET                 \n",
      "א                        ADJ                 \n",
      "# # נ ו                  ADJ                 \n",
      "# # ש י ת                ADJ                 \n",
      "ה ר                      _                   \n",
      "# # ג י ל                _                   \n",
      "# # ה                    _                   \n",
      "ה                        DET                 \n",
      "ר                        ADJ                 \n",
      "# # ג י ל                ADJ                 \n",
      "# # ה                    ADJ                 \n",
      ",                        PUNCT               \n",
      "ש ב י ן                  _                   \n",
      "ש                        SCONJ               \n",
      "ב י ן                    ADP                 \n",
      "כ ך                      PRON                \n",
      "ו ב י ן                  _                   \n",
      "ו                        CCONJ               \n",
      "ב י ן                    ADP                 \n",
      "כ ך                      PRON                \n",
      "\"                        PUNCT               \n",
      "א י ן                    VERB                \n",
      "ל ה                      _                   \n",
      "ל                        ADP                 \n",
      "# # _                    ADP                 \n",
      "_                        PRON                \n",
      "# # ה י                  PRON                \n",
      "# # א                    PRON                \n",
      "ש י מ ו ש                NOUN                \n",
      "ל א                      _                   \n",
      "# # ף                    _                   \n",
      "ל                        ADP                 \n",
      "א ף                      DET                 \n",
      "א ח ד                    NUM                 \n",
      "\"                        PUNCT               \n",
      ".                        PUNCT               \n"
     ]
    }
   ],
   "source": [
    "example_batch = train_dataset.as_numpy_iterator().next()\n",
    "\n",
    "for token, label in zip(example_batch[0][\"input_ids\"][0], example_batch[1][0]):\n",
    "    if token == 0:\n",
    "        break\n",
    "    print(\"{:<25}{:<20}\".format(tokenizer.decode(int(token)), tagset[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"../checkpoints_\" + training_lang + \"/\" + model_name + \"_pos_checkpoint.hdf5\", \n",
    "                             verbose=1, monitor='val_ignore_acc',\n",
    "                             save_best_only=True, mode='max', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def ignore_acc(y_true_class, y_pred_class, class_to_ignore=0):\n",
    "    y_pred_class = K.cast(K.argmax(y_pred_class, axis=-1), 'int32')\n",
    "    y_true_class = K.cast(y_true_class, 'int32')\n",
    "    ignore_mask = K.cast(K.not_equal(y_true_class, class_to_ignore), 'int32')\n",
    "    matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[ignore_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=epochs, steps_per_epoch=np.ceil(len(train_examples) / batch_size),\n",
    "          validation_data=dev_dataset, validation_steps=np.ceil(len(dev_examples) / batch_size),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
