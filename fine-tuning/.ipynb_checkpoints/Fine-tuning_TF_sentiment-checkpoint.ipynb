{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_preparation.data_preparation_sentiment import Example, convert_examples_to_tf_dataset, make_batches\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language:   Korean \n",
      "\n",
      "Already trained:     Bulgarian  English  Basque  Finnish  Hebrew  Croatian  Slovak  Thai  Vietnamese  Chinese\n",
      "\n",
      "Not yet trained:     Arabic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_dicts = utils.make_lang_code_dicts()\n",
    "code_to_name = code_dicts[\"code_to_name\"]\n",
    "name_to_code = code_dicts[\"name_to_code\"]\n",
    "    \n",
    "file = open(\"../data_exploration/sentiment_table.txt\", \"r\")\n",
    "all_langs = [line.split(\"&\")[1].strip() for line in file.readlines()]\n",
    "trained_langs = [code_to_name[x.split(\"\\\\\")[1]] for x in glob.glob(\"E:/TFM_CCIL/checkpoints/*/*sentiment.hdf5\")]\n",
    "remaining_langs = [lang for lang in all_langs if lang not in (trained_langs + [\"Turkish\", \"Japanese\", \"Russian\"])]\n",
    "\n",
    "if remaining_langs:\n",
    "    training_lang = remaining_langs[0]\n",
    "    print(\"{:<20}\".format(\"Training language:\"), training_lang, \"\\n\")\n",
    "    training_lang = name_to_code[training_lang]\n",
    "    print(IPython.utils.text.columnize([\"Already trained:   \"] + trained_langs, displaywidth=150))\n",
    "    print(IPython.utils.text.columnize([\"Not yet trained:   \"] + remaining_langs[1:], displaywidth=150))\n",
    "else:\n",
    "    print(\"No languages remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "max_length = 512\n",
    "batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "epochs = 20\n",
    "use_class_weights = False\n",
    "\n",
    "# Model creation\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Checkpoint for best model weights\n",
    "checkpoint_dir = \"E:/TFM_CCIL/checkpoints/\" + training_lang + \"/\"\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint = ModelCheckpoint(checkpoint_dir + model_name + \"_sentiment_checkpoint.hdf5\", \n",
    "                             verbose=1, monitor=\"val_sparse_categorical_accuracy\",\n",
    "                             save_best_only=True, mode=\"max\", save_weights_only=True)\n",
    "\n",
    "# Model compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "path = \"../data/sentiment/\"\n",
    "\n",
    "for dataset_name in [\"train\", \"dev\"]:\n",
    "    # Load and preprocess\n",
    "    dataset = pd.read_csv(path + training_lang + \"/\" + dataset_name + \".csv\", header=None)\n",
    "    dataset.columns = [\"sentiment\", \"review\"]\n",
    "    dataset[\"sentiment\"] = pd.to_numeric(dataset[\"sentiment\"]) # Sometimes label gets read as string\n",
    "    lengths = dataset[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    dataset = dataset[lengths <= max_length].reset_index(drop=True) # Remove long examples\n",
    "    \n",
    "    # Calculate class weights or balance dataset\n",
    "    if use_class_weights and dataset_name == \"train\":\n",
    "        positive_prop = dataset[\"sentiment\"].mean()\n",
    "        class_weights = {0: positive_prop, 1: 1 - positive_prop}\n",
    "    elif not use_class_weights and dataset_name == \"train\":\n",
    "        class_weights = None\n",
    "        positive_examples = dataset[\"sentiment\"].sum()\n",
    "        n = min(positive_examples, dataset.shape[0] - positive_examples)\n",
    "        \n",
    "        if training_lang == \"ar\":\n",
    "            # Testing whether a smaller dataset will work better\n",
    "            n = 2500\n",
    "            \n",
    "        ones_idx = np.random.choice(np.where(dataset[\"sentiment\"])[0], size=n)\n",
    "        zeros_idx = np.random.choice(np.where(dataset[\"sentiment\"] == 0)[0], size=n)\n",
    "        dataset = dataset.loc[list(ones_idx) + list(zeros_idx)].reset_index(drop=True)\n",
    "        \n",
    "    # Convert to TF dataset\n",
    "    dataset = convert_examples_to_tf_dataset([(Example(text=text, category_index=label)) for label, \n",
    "                                                                                             text in dataset.values], \n",
    "                                              tokenizer, max_length=max_length)\n",
    "    if dataset_name == \"train\":\n",
    "        dataset, batches = make_batches(dataset, batch_size, repetitions=epochs, shuffle=True)\n",
    "    else:\n",
    "        dataset, batches = make_batches(dataset, batch_size, repetitions=1, shuffle=False)\n",
    "    \n",
    "    datasets[dataset_name] = (dataset, batches)\n",
    "    \n",
    "train_dataset, train_batches = datasets[\"train\"]\n",
    "dev_dataset, dev_batches = datasets[\"dev\"]\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.5271 - sparse_categorical_accuracy: 0.7435\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.79530, saving model to E:/TFM_CCIL/checkpoints/th/bert-base-multilingual-cased_sentiment_checkpoint.hdf5\n",
      "1680/1680 [==============================] - 1075s 640ms/step - loss: 0.5271 - sparse_categorical_accuracy: 0.7435 - val_loss: 0.4890 - val_sparse_categorical_accuracy: 0.7953\n",
      "Epoch 2/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.3793 - sparse_categorical_accuracy: 0.8458\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.79530 to 0.81882, saving model to E:/TFM_CCIL/checkpoints/th/bert-base-multilingual-cased_sentiment_checkpoint.hdf5\n",
      "1680/1680 [==============================] - 1086s 646ms/step - loss: 0.3793 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.4564 - val_sparse_categorical_accuracy: 0.8188\n",
      "Epoch 3/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.2704 - sparse_categorical_accuracy: 0.8988\n",
      "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.81882\n",
      "1680/1680 [==============================] - 1124s 669ms/step - loss: 0.2704 - sparse_categorical_accuracy: 0.8988 - val_loss: 0.5820 - val_sparse_categorical_accuracy: 0.8092\n",
      "Epoch 4/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.2048 - sparse_categorical_accuracy: 0.9245\n",
      "Epoch 00004: val_sparse_categorical_accuracy did not improve from 0.81882\n",
      "1680/1680 [==============================] - 1107s 659ms/step - loss: 0.2048 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.5695 - val_sparse_categorical_accuracy: 0.7901\n",
      "Epoch 5/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.1472 - sparse_categorical_accuracy: 0.9507\n",
      "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.81882\n",
      "1680/1680 [==============================] - 1100s 654ms/step - loss: 0.1472 - sparse_categorical_accuracy: 0.9507 - val_loss: 0.5732 - val_sparse_categorical_accuracy: 0.8110\n",
      "Epoch 6/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.1071 - sparse_categorical_accuracy: 0.9632\n",
      "Epoch 00006: val_sparse_categorical_accuracy did not improve from 0.81882\n",
      "1680/1680 [==============================] - 1101s 656ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.7337 - val_sparse_categorical_accuracy: 0.8075\n",
      "Epoch 7/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.1060 - sparse_categorical_accuracy: 0.9617\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.81882 to 0.82491, saving model to E:/TFM_CCIL/checkpoints/th/bert-base-multilingual-cased_sentiment_checkpoint.hdf5\n",
      "1680/1680 [==============================] - 1100s 654ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9617 - val_loss: 0.6863 - val_sparse_categorical_accuracy: 0.8249\n",
      "Epoch 8/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.0984 - sparse_categorical_accuracy: 0.9655\n",
      "Epoch 00008: val_sparse_categorical_accuracy did not improve from 0.82491\n",
      "1680/1680 [==============================] - 1092s 650ms/step - loss: 0.0984 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.6969 - val_sparse_categorical_accuracy: 0.8110\n",
      "Epoch 9/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.0831 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.82491\n",
      "1680/1680 [==============================] - 1093s 651ms/step - loss: 0.0831 - sparse_categorical_accuracy: 0.9713 - val_loss: 0.7461 - val_sparse_categorical_accuracy: 0.8040\n",
      "Epoch 10/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.0737 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.82491\n",
      "1680/1680 [==============================] - 1100s 655ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9742 - val_loss: 0.7589 - val_sparse_categorical_accuracy: 0.8049\n",
      "Epoch 11/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.0811 - sparse_categorical_accuracy: 0.9716\n",
      "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.82491\n",
      "1680/1680 [==============================] - 1103s 657ms/step - loss: 0.0811 - sparse_categorical_accuracy: 0.9716 - val_loss: 0.8585 - val_sparse_categorical_accuracy: 0.7988\n",
      "Epoch 12/20\n",
      "1680/1680 [==============================] - ETA: 0s - loss: 0.0683 - sparse_categorical_accuracy: 0.9769\n",
      "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.82491\n",
      "1680/1680 [==============================] - 1102s 656ms/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9769 - val_loss: 0.8136 - val_sparse_categorical_accuracy: 0.8136\n",
      "Epoch 13/20\n",
      "   6/1680 [..............................] - ETA: 14:30 - loss: 0.1092 - sparse_categorical_accuracy: 0.9167"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cd9362ddca47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdev_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32me:\\anaconda\\envs\\tfm_ccil\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=epochs, steps_per_epoch=train_batches, \n",
    "          validation_data=dev_dataset, validation_steps=dev_batches,\n",
    "          class_weight=class_weights,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_dir + model_name + \"_sentiment_checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 56s 195ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(dev_dataset, steps=dev_batches, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.832     0.887     0.859       689\n",
      "           1      0.812     0.732     0.770       459\n",
      "\n",
      "    accuracy                          0.825      1148\n",
      "   macro avg      0.822     0.809     0.814      1148\n",
      "weighted avg      0.824     0.825     0.823      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_data = pd.read_csv(path + training_lang + \"/\" + \"dev\" + \".csv\", header=None)\n",
    "val_data.columns = [\"sentiment\", \"review\"]\n",
    "lengths = val_data[\"review\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "val_data = val_data[lengths <= max_length].reset_index(drop=True)\n",
    "print(classification_report(val_data[\"sentiment\"].values, preds[0].argmax(axis=-1), digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.857     0.914     0.885       689\n",
      "           1      0.857     0.771     0.812       459\n",
      "\n",
      "    accuracy                          0.857      1148\n",
      "   macro avg      0.857     0.843     0.848      1148\n",
      "weighted avg      0.857     0.857     0.856      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_data[\"sentiment\"].values, preds[0].argmax(axis=-1), digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
